\chapter{\color{oxfordblue} Conclusion and Outlook}\label{chap-Conclusion}
\ChapFrame
\vspace{-1cm}
This thesis aims to follow a logical chain, starting from the theory underpinning modern particle physics in Chapter~\ref{chap-theory}. The \gls{sm} has been extensively validated by many experiments across the world, in particular by the ATLAS Collaboration using data collected from proton-proton collisions in the \gls{lhc}, as presented in Chapter~\ref{chapter-ATLAS}. Many properties of the particle discovered in 2012 have been confirmed to correspond to those of the predicted \gls{sm} Higgs boson. The ATLAS Collaboration continues to systematically study the new particle and challenge the \gls{sm} in evermore complex measurements, searching for any discrepancy between the observations and theory. This mission requires state-of-the-art detectors and reconstruction software. \\

At its core, a particle physics analysis is a statistical data analysis that is well suited to modern machine learning and artificial intelligence, as reviewed in Chapter~\ref{Chap-ML}. The rapid progress in this field provides an exciting avenue of development for ATLAS, helping the Collaboration propel the performance of its software to new heights by designing effective network-based models for specific purposes, such as operating the detector, generating simulated samples, reconstructing objects and events, disentangling backgrounds, and statistically modelling the physics processes. \\

One such promising area of progress concerns jet flavour tagging, which has continuously benefitted from the adoption of advanced \gls{ml} in recent years, as outlined in Chapter~\ref{chap-ftag}. \gls{gn2}, the latest generation of taggers of the ATLAS Collaboration, relies on a single multimodal network exploiting a transformer encoder at its core, with multiple tasks targeted to distil expert knowledge in the network. Far from resting on these accomplishements, the ATLAS Collaboration is further pushing the capabilities of \gls{gn2} by optimising its hyperparameters. This represents a significant computational challenge due to scarcity of available \gls{gpu} resources, a difficulty that can be addressed by deploying new techniques from the \gls{ai} community. The state-of-the-art performance delivered by \gls{gn2} will reverbate into more refined measurements from the numerous analyses targetting heavy-flavour quarks in their final state during the ongoing Run 3 of the \gls{lhc}.  \\

Two analyses benefitting starkly from flavour tagging are the \vhb\ and \vhc. They have now been joined into the combined \vhbc\ analysis, described in Chapter~\ref{chap-VH}. At the time of writing, the analysis is in its last phase with final studies on the modelling and the fit framework, hence the results presented here are still blinded. Excitingly however, there are hints of great progress in the effort to observe the $H \rightarrow c\bar{c}$ decay and measure the $c$-quark Yukawa coupling. The expected upper limit on the signal strength has been reduced by a factor of 2.8 to 11.1 $\times$ \gls{sm} expectations, compared to the last published ATLAS result and using the same data samples \cite{Collaboration:2721696}. Similarly, great progress is made in the precision measurement of the $H \rightarrow b\bar{b}$, with an expected signal strength sensitivity of 7.9 $\sigma$ corresponding to a 23\% improvement over the last ATLAS published result \cite{ATLAS:2021wqh}. For the first time, both production modes are expected to be observed at more than 5 $\sigma$ in the $H \rightarrow b\bar{b}$ decay mode, with respective significances of 5.5 $\sigma$ for $WH$ and 6.2 $\sigma$ for $ZH$. A  simplified template cross section measurement of the differential cross sections of \vhb\ is also performed in stage 1.2 \cite{Butterworth:2015oua}. \\

To continue exploring the limit of our understanding in the particle physics realm, algorithmic and machine developments are required across the experiment. Collecting large datasets is crucial to analyses searching for rare signatures and performing precision measurements. The \vhbc\ analysis presented here suffers from large statistical uncertainties that will be improved with the addition of data. Gathering more data at higher energies requires the detector to be operated in more challenging conditions, since more pile-up is the price of a higher instantaneous luminosity. The subdetectors must be upgraded to deal with this increased activity, with in particular finer-grain measurements expected to improve the detector performance. In this regard, the development of the next inner detector system called ITk is a promising avenue \cite{Bortoletto:2022wcx}. \\

Simultaneously to improving the hardware, the software of the ATLAS Collaboration must be upgraded to further push the sensitivity of the detector and deal with the future challenging conditions. In this respect, flavour tagging benefits greatly from adopting advanced new \gls{nn} architecture such as the transformer, but also from the multimodal input and multitask paradigms to nimbly introduce expert knowledge. Future avenues of progress primarily relies on pursuing this path further, adding additional low-level input information and defining additional tasks to help the main classification objective. Performance is highly correlated with the number of parameters, and reliably training larger networks requires careful design, well-thought training procedures, large datasets, and optimised hyperparameters. Across science and industry, advanced machine learning plays a crucial role in the effort to modernise software capabilities. This is particularly the case in \gls{hep}, where the large databases measured from collisions or simulated are effectively exploited to create reliable and precise models. Such networks are trained for all the uses of the field: from generative \gls{ai} to effectively produced simulated samples, to fast network deployed on \glspl{fpga}- or \gls{gpu}-based triggers, \gls{dl} to reconstruct physics objects from the rich noisy set of low-level data, and finally \gls{ml} deploying in analyses to improve signal discrimination from the backgrounds and help constrain the modelling of the different processes. 