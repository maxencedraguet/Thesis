\chapter{\color{oxfordblue} Machine Learning \& Deep Learning}\label{Chap-ML}
\ChapFrame

\textit{
This chapter is dedicated to a review of relevant machine learning and deep learning methods in the context of High Energy Physics. As for other fields of science and technology, the recent advancements in artificial intelligence have introduced a plethora of useful techniques that can be leveraged in particle physics. Before starting the review, the often confounded terminology is defined. It is followed by an overview of the most commonly deployed approaches in particle physics: decision trees and deep neural networks. A final word on optimisation techniques is given at the conclusion of this chapter.
}

\section{Definitions}
\subsection{Artificial Intelligence}
\textit{\glsreset{ai}\gls{ai}} encapsulates any software \textit{program} that aims to mimic an aspect of human intelligence. A non-exhaustive list of these abilities includes: 
\begin{itemize}
    \item \textit{Reasoning}, the ability to conduct logical thoughts and establish their validity.
    \item \textit{Inferring}, the ability to connect logical statements to induce or deduce new statements.
    \item \textit{Creating}, the ability to generate new content or information. 
    \item \textit{Acting}, the ability to perform a task or to modify the direct environment.
\end{itemize}
\gls{ai} research is a large field of investigation that studies these various aspects through numerous subjects such as robotics, \gls{nlp}, computer vision, generative modelling, and \gls{rl} \cite{russel2010}. \\

Artificial intelligence is broadly separated into three levels according to the performance of the underlying system: 
\begin{enumerate}
    \item \textit{Narrow Intelligence} represents artificial intelligence capabilities on a unique task, for which the software is specifically trained or designed. This field includes \textit{reactive \gls{ai}}, where a model is trained to output an optimal decision or prediction based on current conditions only, and \textit{limited-memory \gls{ai}}, where a model can draw knowledge from past experience and build an internal understanding of the problem to make better-informed decisions later. An example of the former is the IBM chess player Deep Blue \cite{CAMPBELL200257}, while the latter is today most famously demonstrated by OpenAI's GPT-4 model \cite{openai2024gpt4}. 
    \item \textit{General Intelligence} refers to an artificial intelligence capable of matching human problem-solving skills in multiple environments. In particular, this hypothetical setting requires machines to learn new tasks on their own and extrapolate from pre-acquired knowledge, a process referred to as \textit{transfer learning}. Such a model would have the ability to adopt and combine several of the traits of intelligence and to generalise the automated learning process to any task.
    \item \textit{Super Intelligence:} describes a hypothetical type of intelligence able to exceed human abilities and exhibit independent control of thoughts. 
\end{enumerate}
Of these, currently, only the first type is attainable and routinely deployed, while the second one is the focus of the most ambitious research in the field of \gls{ai}. \\

The inception of reactive \gls{ai}, the initial approach attempted, comes from the research into games in the 50s and 60s \cite{russel2010}. This paradigm saw the rise of algorithms capable of searching for optimal moves in a large space of possible actions using \textit{heuristics}, human-passed knowledge on useful features of the specific environment of the game. For example, in chess, the point system assigning arbitrary values to each piece helps to decide the worthiness of an action, e.g., a queen is typically worth more than a simple pion. In this reactive approach, neither the rules of the game nor the decision process are learnt. The former is forced into the search logic and the latter is the outcome of the search process. State space exploration of many realistic problems however scales asymptotically with the dimension of the input, quickly rendering reactive approaches impractical. Combined with the need for human-encoded insights into the problem, the potential of reactive \gls{ai} is restricted to specific well-controlled settings with a high degree of human understanding and low environment complexity. \\

Limited-memory \gls{ai} revolutionised the field by removing the need for complete human control of the data interpretation and state formulation, letting instead a well-crafted mathematical model abstract and represent the information internally \cite{russel2010}. It opens the door to applications that are not otherwise realistically tractable, such as autonomous driving, speech recognition, seamless robotics, etc. For such problems, a complete programmatic transcription of the problem formulation is not feasible, which prohibits a reactive approach. The revolutionary paradigm of limited-memory \gls{ai} nowadays outperforms reactive \gls{ai} in all settings (e.g., in chess) and can be exploited in abstract scenarios where heuristics finding is impractical or intractable. For this reason, the focus of this chapter is on limited-memory \gls{ai} as exemplified by machine learning. 

\subsection{Machine Learning} 
\textit{\glsreset{ml}\gls{ml}} underpins the field of narrow \gls{ai} with limited-memory capabilities. It introduced a paradigm shift to the field, moving away from human-declared logic-based rules written in a specific syntax, the hallmark of reactive \gls{ai}. The latter involves the execution of statements such as \[\textrm{\textit{If x happens, do y}},\] for an input $x$ and an output $y$. With limited-memory \gls{ai}, the state representation and decision steps are encoded in the mathematical models of the dataspace ($\mathcal{D}$) and the learning process: \[\forall\, x \in \mathcal{D}, \textrm{ do }f(x) = \hat{y}; \textrm{ update }f(x) \textrm{ given } (x, y),\] where $\hat{y}$ is the prediction of the model. In this case, both the internal representation of the rules and the decision-making are underpinned by the trained mathematical model $f$. Essentially, two distinct steps are applied to the model constructed with adjustable parameters: 
\begin{enumerate}
    \item \textit{Inferring:} the model has to give its prediction $\hat{y}$ on a new data point $x$: $f(x) = \hat{y}$.
    \item \textit{Learning:} the parameters of the model are updated based on a specific training or fitting procedure, depending on whether the training will be progressively exposed to the data points of a training dataset or directly exposed to the entirety of the set. The objective is to align the output of the model $\hat{y}$ with the expected behaviour $y$: given the couple $(x, y)$, let $f(x) = \hat{y} \rightarrow y$ under training convergence. The model $f$ is trained to become an accurate estimator of the label $y$.
\end{enumerate}
The training process closely depends on the type of model being deployed. These can be broadly separated into two groups:
\begin{itemize}
    \item \textit{Classical machine learning:} covers models deploying specific algorithms to exploit the data in a pre-defined and fixed approach. They include linear regression, decision trees, Support Vector Machine (SVM), logistic regression, kernel methods, $k$-Nearest Neighbours, etc.
    \item \textit{\gls{dl}}: these methods are based on a core logical module called the \textit{artificial neuron}. This module is stacked into layers of given widths, meaning a given number of neurons, and several such layers are then connected along its depth. Within this category, the information flow through the network defines different types of \gls{dl}.
\end{itemize}
Deep learning is thus very much a part of machine learning, constituting a specialised approach to building models from the core artificial neuron unit. Classical machine learning techniques prove valuable in many applications thanks to their ease of use and their ability to be trained when data is scarce. \gls{ml} can be deployed on various tasks: 
\begin{itemize}
    \item \textit{Classification:} assigning a discrete variable called \textit{label} to a datapoint: e.g., identifying $b$-jets. The general case is multiclass, with $n$ possible labels, and a particularly common case is binary classification. \textit{Clustering} is an analogous task where similar data must be grouped.
    \item \textit{Regression:} predicting a continuous variable for a datapoint: e.g., \pt\ reconstruction. 
    \item \textit{Features extraction:} given a dataset with specific internal features, construct new features, e.g., reconstructing the secondary vertex from a set of tracks. A special subcase of this category is embedding data points into a hyperspace. The dimension of this final space can be larger, when embedding the data into a richer space, or smaller, in the case of dimensionality reduction. A popular example of the latter is \textit{\gls{pca}}, which projects the data to a subspace spanned by its principal eigenvectors, those associated with the largest eigenvalues.
    \item \textit{Generation:} sampling new data from a distribution matching the training dataset distribution, e.g., sampling new $t\bar{t}$ events from a learnt statistical model. 
    \item \textit{Anomaly detection:} identify and flag rare events in an unlabelled dataset.
\end{itemize}

To perform these different tasks, models are constructed following different paradigms of \gls{ml}, divided according to the amount of human intervention \cite{MurphyML}:
\begin{itemize}
    \item \textit{Supervised learning:} the data used for training is endowed with the information the model must learn to predict. In the training step, the model is therefore optimised to make predictions that closely align with the target. Classification and regression are the most common tasks to fall under this realm.
    \item \textit{Unsupervised learning:} the model is trained with an objective to optimise without explicit targets and should discover patterns and insights without guidance. Generative models and clustering are prime examples. 
    \item \textit{Semi-supervised learning:} also called \textit{weak supervision}, is a paradigm combining the supervised and unsupervised approaches. The model is mostly unsupervised but can benefit from some labelled cases, or human input in \textit{active learning}. A prime example is to combine an unsupervised clustering task with a classification of the formed groups. This is particularly fruitful when data labelling costs are expensive, as is the case with real-world data.
    \item \textit{Self-supervised learning:} a machine instructs itself on what tasks to learn. The overarching goal of the model is defined but the learning process generates its own supervisory signals from the data.
    \item \textit{Reinforcement Learning:} this paradigm of \gls{ml} is dedicated to the setting of a game-theoretic environment. An agent - an actor of the game - explores and interacts with the environment by choosing actions from a learnable policy and estimates its current situation and expected reward. In \gls{rl}, the agent must learn to construct the best policy to satisfy a reward function and obtain the best outcome.
\end{itemize}
These different settings are addressed using deep learning, which is widely recognised as the most effective technique currently available, thanks to its ability to scale in complexity while remaining computationally tractable.

\subsection{Deep Learning} 
\textit{\glsreset{dl}\gls{dl}} specifically refers to family of \gls{ml} methods that were predominantly derived in the 1980s and have quickly grown in popularity in the last decade, with widely advertised results on competitive benchmark tasks in pattern recognition, such as the super-human performance of the \textit{DanNet} model \cite{DanNet} based on \glspl{cnn} \cite{NIPS198953c3bce6}. The basis of any deep learning method is the artificial neuron, a logical unit inspired by the human neuron. Several such units are combined into layers of any number of neurons defining the width of the layer, and the layers are stacked, with deeper layers receiving as input the output of earlier layers. Different \gls{dl} models are constructed by modifying the structure of the layers - in particular, the input, output, and activation function used - and the transfer of information between neurons, be that depth-wise between layers or width-wise between neurons. \\

\gls{dl} is well-suited to the needs of the ATLAS Collaboration because:
\begin{itemize}
    \item Large datasets of both real and simulated data are available.
    \item Thanks to advanced \gls{mc} simulation programs of both the physics process and the detector reconstruction, these simulations are faithful representations of the real data.
    \item The data and data model from which the data originates is well understood in physics, the former coming from measurements from well-calibrated detectors and the second from crafted theories of the field. 
    \item The data exhibits rich features due to the different collection patterns of various detectors and the different scales of the underlying physics processes. The available representations span images, sequences, sets, and graphs, aligning with the main data representations studied by the deep learning community.
\end{itemize}
Given how important this form of \gls{ai} has become, this chapter is dedicated to introducing some of the most relevant approaches for \glsreset{hep}\gls{hep}. 

\section{Machine Learning Methods for Particle Physics}
A typical \gls{hep} analysis can be described as a five-step process:
\begin{enumerate}
    \item Data collection: real data is collected from a detector. 
    \item Simulated data is generated to match the condition of collection of the real data in terms of detector effects and operational conditions such as energy, \gls{pu}, and luminosity. This simulated data is built upon our best theoretical knowledge. 
    \item The detectors deployed in modern particle physics experiments are composed of a complex set of subdetectors sensitive to different physical phenomena. The low-level information collected by different devices must be processed and recombined to generate \textit{physics objects}, aggregating low-level information to hold physical meaning, as introduced in Chapter \ref{chapter-ATLAS}.
    \item An analysis strategy is established, to similarly restrict the full datasets of both simulated and real data to a portion of the data that is most sensitive to the studied \textit{signal} or process.
    \item With the optimally selected set of real and simulated data points, a statistical model is built to quantify the agreement of the measured data with the expectations from the theory under the conditions of the experiment. This is often achieved through a profile likelihood fit, where the parameters of interest targeted by the analysis are measured to be those maximising the likelihood under the given measured data.
\end{enumerate}

Modern advanced machine learning has the potential to \textbf{improve all steps} of this process:
\begin{enumerate}
    \item The operational side of running the detector and the accelerators can benefit from \gls{rl} methods for improved control of the different electronic devices and online data quality monitoring. Triggers, an essential component of the ATLAS experiment described in Chapter~\ref{sub-sec-trigger}, can be upgraded to use sophisticated \gls{dl} models running online thanks to a hardware backbone built on \glspl{fpga} or \glspl{gpu}.
    \item Simulating a Monte Carlo dataset is a computationally intensive task. Generative \gls{ai} has the potential to accelerate this step by providing a statistical model that can be efficiently sampled. \gls{gan} and \gls{vae} have been shown to competitively perform the sampling step. However, a key current limitation of these approaches is the difficulty in fully incorporating the sophisticated theoretical model required to simulate the data, as any discrepancy or non-closure introduces levels of disagreements that are counter-productive in the final physics analysis.
    \item \gls{ml} is particularly well-suited for object reconstruction. Broadly, machine learning offers scalable, efficient, and accurate techniques for this essential task. Important examples in ATLAS are particle identification (e.g., $\tau$ identification), \etm\ reconstruction, and heavy-flavour jets classification, as demonstrated in Chapter~\ref{chap-ftag}.
    \item Historically, physicists have relied on a cut-based approach to selecting data. For example, in a leptonically decaying $Z$ boson measurement to two charged leptons $\ell^+\ell^-$, restricting the invariant mass of the lepton pair $m_{\ell^+\ell^-}$ to lie close to the $Z$ boson rest mass is beneficial to select this process. Machine learning entirely bypasses this process, learning directly from an appropriate set of signal and background samples to optimise the separation of the signal from the background. 
    \item The likelihood function of the constructed statistical test, quantifying the level of agreement between the real data and the theory through the simulated sample, can be directly learnt by a model given access to both sets. Additionally, anomaly detection, such as the search for unknown resonances, can be automated with unsupervised machine learning. 
\end{enumerate}

Contributing to step 3 in the aforementioned list is of the main focus of Chapter \ref{chap-ftag} of this thesis: developing \gls{dl} tools for improved jet classification. The analysis presented in Chaptet \ref{chap-VH} also introduces some classical \gls{ml} techniques of data selection as suggested in step 4. 

\subsection{Decision Trees}
\textit{\glspl{dt}}, also known as Classification and Regression Trees (CART), are the bread and butter of any data analysis. They are simple to train, perform well for both classification and regression tasks, and are interpretable. The model works by recursively partitioning the input feature space \cite{MurphyML}. Each partition creates a node, forming a tree structure. The initial node, called the root, is sequentially split into branches, with each branch leading to a leaf node representing a final output region. The splits are based on the features of the input data, which can be either discrete categorical values (e.g., lepton labels such as $e, \mu, \tau$) or continuous values (e.g., $m_{\ell^+\ell^-}$). Here is a simple example of a classification tree that predicts the class as either 0 or 1:

\Tree[.\textit{$x_i \leq c_i$} [.{True \\\textit{$x_j \geq c_j$}} [.True 1 ]
            [.False 0 ]]
        [.{False \\\textit{$x_k \leq c_k$}} [.True 0 ]
            [.{False \\{\textit{$x_l$} is \textit{$e$}}} [.True 0 ]
                            [.False 1 ]]]]

At each node, a condition is learned with $x_i, x_j, x_k$ representing continuous features of the dataset that are cut at the thresholds $c_i, c_j, c_k$, and $c_l$ representing a categorical feature (e.g., whether the lepton is an electron). The leaf values represent the output of the tree in different regions defined by the combination of successive selections. In this example, a binary variable indicating the class is used. Figure~\ref{fig:tree-ex} shows an example of a classification tree, where a tree with two nodes can isolate most of the blue class from the red class within the region limited by the green lines, which correspond to the conditions $x_1 \geq c_2$ and $x_2 \geq c_2$ being satisfied.

\begin{figure}[h!]
    \center
    \begin{minipage}[c]{0.35\textwidth}
        \caption{A binary classification problem with two features. A decision tree applies two successive cuts $c_1$ and $c_2$ to isolate most of the blue class from the red.}\label{fig:tree-ex}
      \end{minipage}
      \begin{minipage}[c]{0.5\textwidth}
        \includegraphics[width=\textwidth]{Images/ML/scatterPlot.png}
      \end{minipage}
\end{figure}
    
\paragraph{}In terms of complexity, finding the optimal set of partitions for a dataset is an NP-complete problem, making it intractable for large datasets. Instead, a greedy method is adopted to build a tree, using a heuristic approach to find a satisfactory solution \cite{MurphyML}. A common approach is to choose the most optimal step at each stage, without guaranteeing a global optimum, but rather a local one. The chosen split is selected based on a defined cost function, given by
\begin{equation}\label{eq:DTcost}
    (j^*, t^*) = \arg\min_{j\in \{1, ..., D\},\, t \in T_j} \min \left(\text{cost} (\{x_i, y_i : x_{ij} \leq t\}) + \text{cost}(\{x_i, y_i : x_j > t\}) \right),
\end{equation}
where $T_j$ is the set of possible thresholds, and $x_j$ and $y_j$ are the features and labels (or regressive objectives). The \textit{cost} function depends on the objective of the tree, with the regression case typically using the \gls{mse} error function \[cost(D) = \sum_{i\in D}(y_i - \bar{y})^2,\] and the loss is often one of the following for classification \cite{MurphyML}:
\begin{itemize}
    \item \textit{Misclassification rate:} $\frac{1}{|D|} \sum_{i \in D} \mathbb{I}(y_i \neq \hat{y})$, where $D$ is the data in the leaf of the tree and $\mathbb{I}$ is the indicator function defined as $\mathbb{I}(x) = 1$ if $x$ is True, else $0$. 
    \item \textit{Statistical entropy:} defining the class conditional probability as $\pi_c = \frac{1}{|D|} \sum_{i \in D} \mathbb{I}(y_i \neq c)$, the entropy over the $C$ classes is defined as
    \begin{equation}\label{eq:statEntropy}
        H(\boldsymbol{\pi}) = - \sum_{c=1}^C \pi_c \log \pi_c,
    \end{equation}
    with $\boldsymbol{\pi}$ a vector ($\pi_1, \pi_2, ..., \pi_C$) of the class-condition probabilities.
    \item \textit{Information Gain:} an equivalent formulation to the entropy, measuring the gain in information from the change in entropy induced by adding a selection on feature $X_j$ to the current selection 
    \[ \text{Gain}(X_j < t, Y) = H(Y) - H(Y | X_j < t) \]
    \item \textbf{Gini:} computes and minimises the expected error rate:
    \begin{equation}\label{eq:giniClass} 
        \sum_{c=1}^C \pi_c (1 - \pi_c).
    \end{equation}
\end{itemize}

The pseudocode algorithm to train a \gls{dt} with the update rule of Equation \ref{eq:DTcost} is summarised in Algorithm \ref{ag:DT} of Appendix \ref{app:MLalgo}. \\

\glspl{dt} can overfit a dataset when the model tunes itself to specific features of the training set that do not generalise well. Regularisation serves as an important step to prevent this undesirable behaviour. One approach is to interrupt the growth of the tree when it is no longer beneficial to continue, or to \textit{prune} the tree by removing nodes or branches that contribute little to the overall performance. This helps prevent the tree from becoming too complex and overfitting the data. Another technique is to train multiple trees with different random subsets of the data and aggregate the results into a single prediction. By averaging the predictions of multiple trees, the overall performance can be more reliable and less prone to overfitting. For regression tasks, the predictions of the individual trees are averaged, while for classification tasks, the predicted class can be determined by majority voting. This statistical technique of combining different predictors is referred to as \textit{bagging} or \textit{ensembling}. The different predictors can be built on subsets of the input features and training datapoints, thereby forming a \textit{random forest} \cite{russel2010}. Random forests are a popular ensemble method that combines the strengths of multiple decision trees to create a more robust and accurate model. They are widely used in various machine learning applications and have proven to be effective in reducing overfitting and improving generalisation performance.

\subsection{Boosted Decision Trees}
A popular extension to the simple \gls{dt} approach is to introduce the concept of \textit{boosting}, leading to a technique referred to as \textit{\glsreset{bdt}\gls{bdt}} or \textit{\glsreset{mva}\gls{mva}}. Boosting is a greedy algorithm leveraging a weak learner and applying it sequentially to weighted versions of the data, with a larger weight given to misclassified datapoints. The method is hugely popular in data science, having earned the title \textit{``best off-the-shelf classifier in the world''} \cite{baggingML}. Two particularly useful approaches are adaptive boosting (AdaBoost) \cite{Adaboost} and gradient boosting \cite{gradientBoosting}, both combining an ensemble of $M$ weak learners $f_i$ ($i = 1, ..., M$) into a strong learner $F$: \[F(x) = \sum_{i=1}^M f_i(x).\] For the following discussion, the model is built using a training dataset $ \{(x_1, y_1), ..., (x_N, y_N)\}$ with input vectors $x_i \in \{\mathbb{R} \otimes \mathbb{D}\}^d$ of $d$ features that are real or discrete ($\mathbb{D}$) and $y_i \in \mathbb{R}^d$ is a $d$-dimensional real vector that serves as output to be predicted by the model.

\subsubsection{AdaBoost}\label{sub-adaboosted}
\textit{AdaBoost} combines the $M$ weak learners $f_i$ with adaptive weights $\alpha_i$ to improve the ensemble performance as \[F(x) = \sum_{i=1}^M \alpha_i f_i(x),\] where $F$ is the boosted model, and the successive boosting stages $F_T = \sum_{i=1}^{T \leq M} \alpha_i f_i(x)$ define stronger combinations of the weak learners $f_i$ with weights $\alpha_i \in \mathbb{R}$. At each iteration $m$ of the training process ($m = 1, ..., M$), a weak learner $f_m$ is fitted to the training set to minimise a loss function $L(y_i, F_{m}(x_i))$. AdaBoost relies on the exponential loss
\begin{equation}\label{eq:adaboosterror}
    L(y, F_m(x)) = \sum_{i=1}^N \exp\left(-y_i F_m(x_i)\right) = \sum_{i=1}^N \exp\left(-y_i (F_{m-1}(x_i) + \alpha_m f_m(x_i))\right),
\end{equation}
that the new weak learner $\alpha_m f_m$ added at step $m$ has to minimise. The typical case for AdaBoost is binary classification with $y_i \in \{-1, 1\}$, but the algorithm is generalisable to multi-class \cite{MurphyML}. Equation \ref{eq:adaboosterror} can be re-expressed as: \[\sum_{i=1}^N w_{i,m} \exp\left(-\alpha_m y_i f_m(x_i)\right),\] where $w_{i,m} = \exp\left(-y_i F_{m-1}(x_i)\right)$ is interpreted as a weight applied to the datapoint $(x_i, y_i)$  indexed by $i$ at step $m$ proportionally to the error of the current strong learner. One can show that the weak learner $f_m$ minimising the optimisation objective at step $m$ is the one minimising the miss-classified weights sum error $\epsilon_m$ of the reweighted version of the dataset with weights $w_{i,m}$ \cite{MurphyML}, where \[\epsilon_m = \sum_i w_{i,m} \mathbb{I}(y_i \neq f_m(x_i)).\] For the first step $m = 1$, the weights are initialised to $1 / N$. They are then updated to \[w_{i,m+1} = w_{i,m} e^{-\alpha_m y_i f_m(x_i)},\] and renormalised so that $\sum_i w_{i, m+1} = 1$, before being assigned to each training input in the next step. The weak learner is combined with the strong learner using an optimal weight $\alpha_m$ found by minimising the loss $L$ of the combined learner \[\alpha_m = \frac{1}{2} \log \frac{1 - \epsilon_m}{\epsilon_m},\] giving the overall update rule
\begin{equation}\label{eq:updateOverall}
    F_m(x) = F_{m-1}(x) + \alpha_m f_m(x)
\end{equation}
that combines the new weak learners $f_m$ with optimal weight $\alpha_m$ to the current strong learner $F_{m-1}$. The AdaBoost algorithm is summarised in Algorithm \ref{algo:adaboost} of Appendix \ref{app:dtalgo}.

\subsubsection{Gradient boosting}\label{sec-gradient-boost}
\textit{Gradient boosting} is a generic approach which, unlike AdaBoost, is not restricted to a specific loss function. The objective to minimise is the empirical risk, the expected value of the loss function $L$ on the training set 
\begin{equation}\label{eq:empRisk}
    \hat{f}  = \arg \min_f \mathbb{E}_{x,y} L(y, f(x)).
\end{equation}
As the name suggests, the approach leverages gradient descent to find the optimal $\hat{f}$. At step $m$, the gradient of the loss $L$ is evaluated at $f = f_{m-1}$ as \[ g_{i,m} = \left[ \frac{\partial  L(y_i, f(x_i))}{\partial f(x_i)} \right]_{f= f_{m-1}}, \] which is then used to update the learner with a step $f_m = f_{m-1} - \alpha_m g_{m},$, where $g_m$ is the gradient of each datapoint and the step-length $\alpha_m$ is chosen to minimise the residual loss $L(y, f_{m-1}$ $- \alpha_m g_{m})$. This implements functional gradient descent and leads the model to fit the $N$ datapoints of the set. This procedure naturally leads to overfitting, an undesirable feature that is remedied by using a weak learner to approximate the negative gradients. In the specific case of gradient-boosted decision trees, at step $m$ a decision tree $h_m(x)$ is fitted to the pseudo-residuals $g_{i,m}$. This \gls{dt} $h_m$ at step $m$ defines $J_m$ disjoint regions through its leaves with predictions $b_{jm}$ in each region indexed by $j = 1, ..., J_m$: \[ h_m(x) = \sum_{j=1}^{J_m} b_{jm} \textbf{1}_{R_{jm}}(x),\] where $\textbf{1}_{R_{jm}}(x)$ is the indicator function - equal to 1 when $x \in R_{jm}$ and 0 otherwise. The model update is \[f_m(x) = f_{m-1} + \alpha_m h_m(x),\] with $\alpha_m$ selected by minimising the empirical risk \[ \alpha_m = \arg \min_{\alpha} \sum_{i=1}^N L\left(y_i, f_{m-1}(x_i) + \alpha h_m(x_i)\right).\]

The full algorithm for gradient boosting is presented in Algorithm \ref{algo:gradient_boosting} of Appendix \ref{app:dtalgo}, where the update rule is added a \textit{learning rate} hyperparameter $LR$ to introduce regularisation and reduce the risk of overfitting. By keeping $0 < LR \leq 1$, the ability of the model to fully adapt to the training error is limited, thereby improving generalisation to unseen data. The price is a slower updating of the model and a higher computational complexity. Further regularisation techniques are bootstrap aggregation - training each weak learner on a random subset of the data -, limiting the number of leaves, penalising models of larger complexity, and pruning branches that do not sufficiently reduce the loss. \\

\glspl{bdt} resist better to overtraining thanks to the regularisation effect. An undesirable feature of boosting is the loss of direct interpretability of the decision-making process. However, this is more than compensated by an appreciable gain in the performance of the underlying model. An interesting property exhibited by all tree-based algorithms and many \gls{ml} approaches is the ease of quantifying the impact of a specific feature on the result. This technique of \textit{feature importance} assigns a score to each input feature, typically the Gini importance of Equation \ref{eq:giniClass}. Another popular technique, taken from the field of cooperative game theory, is the Shapley value, measuring the average marginal contribution of each feature to the objective function \cite{shapley:book1952, Rozemberczki2022TheSV}.


\subsection{Artificial Neurons}
The \textit{artificial neuron} or \textit{perceptron}, as initially named by its inventor Frank Rosenblatt in his seminal 1958 paper \cite{rosenblatt1958perceptron}, is the logical unit at the core of modern deep learning. Notably, \glspl{dnn} are obtained by stacking layers of artificial neurons. Inspired by biological principles, the perceptron, shown in Figure~\ref{fig:annModel}, accepts multiple inputs and gives as output 1 if the combination of inputs exceeds a certain modifiable threshold, otherwise 0. This combination accepts weights to scale the input which are modified during training to correct the output of the perceptron. Artificial neurons are a direct generalisation on this principle, with the output no longer subject to a threshold but passed through a chosen \textit{activation} function $f$ after being added to a learnable bias term $b$. 

\begin{figure}[h!]
    \center
    \includegraphics[width=0.8\textwidth]{Images/ML/ann.png}
    \caption{An artificial neuron: the inputs $x_i$ ($i= 1, ..., N$) are multiplied by learnable weights $w_i$, summed and added to a learnable bias $b$ and passed to an activation function $f$.} 
    \label{fig:annModel}
\end{figure}

\paragraph{} The interest in artificial neurons stems from a significant theoretical result: stacks of artificial neurons are \textit{universal function approximators} \cite{universalFuncApproxNN,HORNIK1989359}, as shown in the next section. This theoretical result is built on a mathematically advantageous function chosen for $f$: the sigmoid $\sigma$, illustrated in Figure~\ref{fig:sigmoid} and defined as
\begin{equation}\label{eq:sigmoid}
    \sigma(x) = \frac{1}{1 + e^{-x}}.
\end{equation}
\begin{wrapfigure}{r}{0.5\textwidth}
    \begin{center}
        \includegraphics[width=0.4\textwidth]{Images/ML/sigmoid.png}
        \caption{The sigmoid function $\sigma$.} 
        \label{fig:sigmoid}
    \end{center}
\end{wrapfigure}

Thanks to its property to map the set of real numbers to the [0, 1] range, this activation function is often used for numerical stability and probability distribution mappings. An essential mathematical property of the sigmoid, particularly relevant for \gls{dl}, is the ease of computing its derivative: \[\sigma^\prime(x) = \sigma(x) (1- \sigma(x)).\]

The strength of artificial neurons stems from their ability to be efficiently combined into ordered structures with powerful representational expressivity. For an input $x \in \mathbb{R}^d$, an individual neuron applies an affine transformation $f(W_i^T x + b_i)$, where $W_i \in \mathbb{R}^d,\,b_i \in \mathbb{R}$ are the weights and bias of the neuron $i$, with the result passed through an activation function $f$. Combining these operations leads to a mathematical model capable of approximating any continuous function, as shown in the next section. 

\subsection{Deep Neural Networks}
A \textit{Deep Neural Network (\gls{dnn})} - also called Multilayer Perceptron (\gls{mlp}), \gls{ann}, feed-forward neural network, or sometimes only \gls{nn} - is constructed by stacking layers of artificial neurons as shown in Figure~\ref{fig:neuralnet}. Each neuron in a layer receives as input the output of the neurons of the previous layer, and connects to the neurons of the next layer. Layers of artificial neurons that are placed between the input and output ones are said to be \textit{hidden layers}. The particularity of the design underpinning this architecture is that layers of neurons connect to all neurons of the next layers only, defining a feed-forward computational graph flowing from the input $x$ to the output $y$. Mathematically, a single layer at depth $i$ with $m$ units, given the previous layer at depth $i-1$ with $n$ neurons as input, computes an affine transformation
\begin{equation}\label{eq:feedforward}
    a^i = f^i\left({W_i}^T a^{i-1} + b_i\right),
\end{equation}
where $W^i \in \mathbb{R}^{m \times n}$ is the matrix of learnable weights of layer $i$ - one row per unit of layer $i$, one column per unit of layer $i-1$, $b_i \in \mathbb{R}^m$ is the vector of learnable biases, $f^i$ is the activation function of layer $i$, and $a^{i-1} \in \mathbb{R}^n$ is the vector of the $n$ activated outputs of the previous layer. The activations can differ for the units of the same layer but are often kept similar to vectorise the mathematical operations. 

\begin{figure}[h!]
    \center
    \begin{minipage}[l]{0.38\textwidth}
        \caption{A deep neural network with $d$ layers of width $m$, $l$, ..., $k$. Each artificial neuron, represented by a ball of darkening shades of blue along the depth, computes an affine transformation of the input of the layer followed by an activation function. The input of the \gls{dnn} is $x$ and the output is $y$.} 
    \label{fig:neuralnet}
      \end{minipage}
      \begin{minipage}[c]{0.6\textwidth}
        \includegraphics[width=\textwidth]{Images/ML/neuralnet.png}
      \end{minipage}
\end{figure}

\paragraph{}Neural networks implement a recursive system of computation based on Equation~\ref{eq:feedforward}. A powerful theoretical property of neural networks is their capacity to be \textit{universal function approximators}. This family of theorems, defined for various types of activations and networks, demonstrates that neural networks built with appropriate activation functions and sufficient capacity can approximate most well-behaving functions \cite{universalFuncApproxNN,HORNIK1989359, universApproximator-Relu}, as presented in Appendix \ref{app:dnn_uni_approx}. This result only requires the activation function to be sigmoidal or discriminatory in the sense
\begin{equation}
    \sigma(x) \rightarrow
    \begin{cases}
        1 \text{ if } x \rightarrow \infty,  \\
        0 \text{ if } x \rightarrow -\infty,
    \end{cases}
\end{equation}
which is satisfied by the sigmoid function. Additionally, the function applied to the neurons should possess some degree of \textit{non-linearity}. A neural network with strictly linear functions would indeed collapse into a linear regression model. Such a non-linear function, when applied to the output of an artificial neuron, is said to \textit{activate} it and is hence called \textit{activation functions}. Some common activation functions, shown in Figure~\ref{fig:commonAct}, are:
\begin{itemize}
    \item The sigmoid function of Equation \ref{eq:sigmoid}.
    \item The hyperbolic tangent function tanh$(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$.
    \item The \gls{relu} function\cite{agarap2019deep}
    \begin{equation}\label{eq:relu}
        \text{ReLu}(x) = \text{max}(0, x).
    \end{equation}
    The non-linearity here is strictly between positive and negative inputs, making this activation function the simplest that can be leveraged in \gls{dnn}. A popular generalisation of \gls{relu} is leakyReLU $= \max(\alpha x, x)$, which introduces a linear function in the negative range controlled by a parameter $\alpha \in [0, 1[$. 
    \item The Exponential Linear Unit (ELU) function modifies the leakyReLU in the negative domain while keeping the saturation property as
    \begin{equation}\label{eq:elu}
        \text{ELU}(x) = 
        \begin{cases}
            x \quad \text{ if } x >= 0, \\
            \alpha (e^x - 1) \text{ otherwise},
        \end{cases}
    \end{equation}
    with the hyperparameter $\alpha > 0$.
    \item The softmax function which, for an $x \in \mathbb{R}^n$, return a vector softmax$(x) = [..., \frac{e^{x_i}}{Z}, ...]$ with $Z = \sum_i e^{x_i}$. The softmax and sigmoid functions are equivalent when $x$ is 2-dimensional. In $n$-dimension, it maps each entry of $x$ to the range $[0, 1]$ and guarantees $\sum_i $softmax$(x)_i = 1$. The softmax can therefore be used to define probability distributions over multidimensional outputs.
\end{itemize}
\begin{wrapfigure}{R}{0.5\textwidth}
    \begin{center}
        \includegraphics[width=0.45\textwidth]{Images/ML/activations.png}
        \caption{The most common non-linear activations used in deep learning.} 
        \label{fig:commonAct}
    \end{center}
\end{wrapfigure}

The sigmoid is no longer the choice of reference, due to its tendency to quickly saturate - meaning its gradient for large positive or negative values \textit{vanishes} by tending to 0. The hyperbolic tangent offers larger gradients thanks to its [-1, 1] range with steeper curvature, making it the activation of choice for autoregressive architectures such as the \gls{rnn}. The \gls{relu} function is the most widely used activation function as its derivative is trivial and does not suffer from vanishing gradients for positive values. Its fixed 0 value for negative input is a double-edged sword: on one side, it helps the network regularise itself by deactivating neurons, on the other some neurons can be stuck in \textit{off}-mode. For this reason, it is important to correctly initialise the weights of a \gls{dnn}. A workaround for this limitation is to introduce leakage for the negative inputs with leakyReLU or ELU activations.\\

While the universal function approximator theorems are a powerful endorsement of neural networks, they do not state \textit{how} to derive the best network. The task of choosing the right architecture in terms of depth and width and the correct weights and biases is approximated by a learning strategy updating the parameters to reduce the error as measured by the empirical risk. What sets apart neural networks from other universal function approximators is the simplicity of the procedure to update their weights: with a suitable computational structure and activation function choice, \glspl{nn} are \textit{differentiable}. Gradients can therefore be computed from a loss function measuring the quality of the output $\hat{y}$ and \textit{backpropagated} across the neurons to update the weights and biases. The recent rise of deep learning can be traced back to improvements in performing this backpropagation of the gradients with publicly available software, such as PyTorch \cite{pytorch} and TensorFlow \cite{tensorflow2015-whitepaper}, implementing efficient algorithms for this essential step. \\

There are two main difficulties encountered when optimising a neural network: the non-convexity of the objective function means saddle points and local minima are abundant, and the computational complexity due to the large number of parameters makes a single update using a large dataset expensive. The large number of parameters implemented by neural networks requires a large dataset to correctly assign values to the parameters without suffering from overtraining. The \textit{backpropagation} algorithm of Algorithm \ref{ag:backpropagation} in Appendix \ref{app:back_pro} circumvents these problems \cite{backprop} by efficiently computing \[ \frac{\partial\mathcal{L}(x, \theta)}{\partial\theta} = \sum_{i=1}^N \frac{\partial l_i(x_i, \theta)}{\partial\theta},\] where $\theta$ encapsulates all parameters of the model, with a loss $l_i$ per datapoint $x_i$. It starts with a forward pass through the network before computing the gradient of each layer from the output and applying the chain rule of calculus. Once all the local gradients are available, the parameters are updated to reduce the loss by taking a step in the direction opposite the gradients, giving the largest reduction in loss. For example, for a specific parameter $w_{ij}$ at training step $t+1$:
\begin{equation}\label{eq:gradientdescent}
    w^{t+1}_{ij} \leftarrow w^{t}_{ij} - LR \times \text{grad}\left[w^{t}_{ij}\right],
\end{equation}
where the \textit{learning rate} $LR$ controls how large a step is taken in the opposite direction of the gradient. Since the gradient of the earlier layers will be derived from the gradient of later layers, the gradients need to respect some numerical stability to avoid the risk of vanishing ($\rightarrow$ 0) or exploding ($\rightarrow \infty$). This requires some care in the architecture choice and can motivate the use of a specific activation function over another. Concerning the loss function $\mathcal{L}$, some typical choices are:
\begin{itemize}
    \item The cross-entropy loss function, based on the definition of entropy in Equation~\ref{eq:statEntropy}. It is commonly used to assign probabilities in a classification problem with $c \in C$ classes: \[ -y_i \log\hat{y}_i,\] where $y_i$ is the real label of the datapoint and $\hat{y} \in [0, 1]^C$ is the model prediction, such that $\sum_i \hat{y}_i = 1$. Given the requirements of the output, it is typically combined with a softmax. 
    \item The \glsreset{mse}\gls{mse} and Mean Absolute Error (MAE) \[\frac{1}{N}\sum_i^N (y_i - \hat{y}_i)^2 \quad \text{and} \quad \frac{1}{N}\sum_i^N |y_i - \hat{y}_i|.\] 
\end{itemize} 

\begin{comment}
\paragraph{Pros:}
\begin{itemize}
    \item \textit{Universal Function Approximators:} neural networks are universal function approximators. With enough parameters, they can approximate any continuous function to arbitrary precision.
    \item \textit{Flexible Architecture:} the architecture of \glspl{nn} is flexible, allowing for extensive customisation of the number of layers, number of neurons, and the activation function. This flexibility makes them suitable for various tasks.
    \item \textit{Feature Learning:} neural networks automatically learn hierarchical representations of features from the input data. They can therefore capture complex non-linear features.
    \item \textit{Availability of Optimisation Techniques:} optimisation techniques built on gradient descent are well-suited for training neural networks. 
\end{itemize}

\paragraph{Cons:}
\begin{itemize}
    \item \textit{Limited Modeling of Sequential and Geometrical Data:} these networks are not naturally designed for handling sequential data and temporal dependencies nor images.
    \item \textit{Fixed Input Size:} they require a fixed input size. While techniques like padding or resizing can be employed, these might not be suitable for handling inputs of varying lengths.
    \item \textit{Lack of Memory:} neural networks do not have an inherent memory mechanism.
    \item \textit{Vanishing and Exploding Gradients:} training \glspl{dnn} can be challenging due to vanishing and exploding gradients.
    \item \textit{Need for Sufficient Labelled Data:} \glspl{dnn} require a large amount of data for effective training. In domains where data is scarce, the performance may be limited.
\end{itemize}
\end{comment}

This section introduces deep neural networks, the fundamental architecture constituted of artificial neurons stacked into layers. In the next sections, refinements to the basic \gls{dnn} are discussed.

\subsection{Recurrent Neural Networks}\label{sec:RNN}
\textit{Recurrent Neural Networks (\gls{rnn})} are an important modification of \glspl{dnn}, derived to work with sequences such as those occurring in \gls{nlp}. The main adaptation from the \gls{dnn} architecture is in the way information is passed through the network: \gls{rnn} are autoregressive models. The information flow is bidirectional: the computation sequentially processes the input at a given step along with the output of the prior step. The advantage of this representation is that the cyclical flow can be unfolded into a directed acyclic computational graph that, for a given sequence length, is equivalent to a \gls{dnn} tailored to the specific length of the input seen. Figure~\ref{fig:rnnNet} presents the structure of an \gls{rnn}-based network as well as its unfolding. The input $x$ is a sequence of $N$ tokens, and the length of different inputs $x_i$ in the dataset can vary. The mathematical structure implemented by this architecture to generate an output $y$ of length equal to the input is
\begin{equation}\label{eq:rnnModel}
    y_t = W(h_t) = W(V(x_t) + U(h_{t-1})),
\end{equation}
where $U$ and $V$ are \gls{dnn} mappings taking at timestep $t$ respectively the previous \textit{hidden state} $h_{t-1}$ and the current input token $x_t$, with $W$ mapping the new hidden state $h_t = V(x_t) + U(h_{t-1})$. The initial hidden state $h_0$ is usually initialised from a special mapping from the whole input $x$.

\begin{figure}[h!]
    \center
    \includegraphics[scale=0.5]{Images/ML/rnn.png}
    \caption{A recurrent neural network, using 3 feed-forward neural networks (DNN) $U$, $V$, $W$, to map the input sequence $x = [x_1, x_2, ..., x_N]$ to the output $y = [y_1, y_2, ..., y_N]$ using the internal hidden state $h^t$ evolving for each timestep $t$. $h_0$ would typically be obtained by a mapping of the whole input sequence $x$. } 
    \label{fig:rnnNet}
\end{figure}

An interesting feature of such a network is its ability to build an internal memory of previous inputs up to a timestep $T$ due to the chain of hidden states $h_{t<T}$. The tanh function is often used as activation in \gls{rnn} to avoid having exploding or vanishing gradients, thanks to its smooth distribution and limitation to the range $[-1, 1]$. As the network relies on repetitive multiplications of numbers in the range $[-1, 1]$, the effect of much earlier timesteps ($h_{t<<T}$) is lost when processing later input at $T$. This process is referred to as \textit{memory loss}. This undesired property is remedied with architectural modifications to \glspl{rnn} that improve their operational memory, such as the \textit{\gls{lstm}}. \\

As shown in Figure~\ref{fig:lstmCell}, am \gls{lstm} cell implements a specific architecture to propagate information along the sequence, with the introduction of a new \textit{control state}  $c$ \cite{lstmPaper}. Three gates regulate the flow of information through the cell. The \textit{forget gate} $F$ decides what information to keep from prior states, by multiplying these values by a factor of 1 and discarding the rest through multiplication by a factor close to 0. The \textit{input gate} $I$ is tasked with creating the new internal state of the cell and determining what information to store in it. Finally, the \textit{output gate} $O$ decides what information in the cell should be passed to the output. This selectivity of the \gls{lstm} cell in deciding what to use from memory, what to keep in memory, and what to output gives this architecture a greatly improved memory for long sequences, resulting in much-improved efficiency. 

\begin{figure}[h!]
    \center
    \includegraphics[scale=0.5]{Images/ML/lstm.png}
    \caption{An LSTM cell. Lines that merge imply concatenation of the inputs, the $\times$, $+$, and tanh are element-wise operations, and the $\sigma$ are different layered transformations (1-layer neural network). $F_t$ is the forget gate, $I_t$ the input gate, and $O_t$ the output gate. } 
    \label{fig:lstmCell}
\end{figure}

%\subsubsection{Gated Recurrent Unit} 
%The Gated Recurrent Unit (\gls{gru}) is another advantageous design to improve the %memory of a recurrent-like network without using as many parameters as an \gls{lstm} %\cite{gruPaper}. There is no output gate and only one internal hidden state $h_t$ is %required. This architecture comes in different versions, with the fully gated version %presented in Figure~\ref{fig:gruCell}. It only requires two gates: the \textit{update %gate} $Z$ and the \textit{reset gate} $R$. The former lets the model decide how much of %the past information should be kept and the latter is used to forget past information.
%
%\begin{figure}[h!]
%    \center
%    \includegraphics[scale=0.5]{Images/ML/gru.png}
%    \caption{A Gated Recurrent Unit cell (fully-gated version). Arrows and lines that %merge imply concatenation of the inputs, the $\times$, $+$, tanh, and ``1 - '' are %element-wise operations - the last one outputting the vector 1 minus its inputs - %and the $\sigma$ are two different layered transformations (1-layer feed-forward %network) with sigmoid-like activation function. $Z_t$ is the update gate and $R_t$ %the reset gate. } 
%    \label{fig:gruCell}
%\end{figure}

\glspl{rnn} and their modification have been designed for ordered sequence analysis and have had great results in such settings. Ordered sequences are natural in language analysis. The choice to sequentially analyse the tokens of a sequence with memory lets \gls{rnn}-based models operate with the powerful representational flexibility of a Universal Turing Machine \cite{NEURIPS2021_ef452c63}. A significant drawback, however, is the impossibility of fully parallelising the processing of a sequence due to the strict ordering, making \glspl{rnn} expensive models to train. The main motivation behind the transformer design, introduced in Section \ref{sec:transformer}, is to address this crucial weakness.

\begin{comment}
\paragraph{Pros:}
\begin{itemize}
    \item \textit{Sequential Processing:} \glspl{rnn} are designed to handle sequential data, making them suitable for tasks with temporal dependencies.
    \item \textit{Flexibility:} \glspl{rnn} can operate on input sequences of variable length.
    \item \textit{Memory:} \glspl{rnn} have a memory mechanism that allows them to retain information about previous inputs.
\end{itemize}

\paragraph{Cons:}
\begin{itemize}
    \item \textit{Vanishing and Exploding Gradients:} Training deep \glspl{rnn} can suffer from vanishing and exploding gradient problems.
    \item \textit{Limited Short-Term Memory:} Traditional \glspl{rnn} struggle to capture long-range dependencies due to their limited short-term memory. 
    \item \textit{Complexity:} While the \gls{lstm} architecture can mitigate the two points above, the cost is a more complex architecture that is harder to train. 
    \item \textit{Interpretability:} The internal workings of a \gls{rnn} is challenging to interpret.
\end{itemize}
\end{comment}

\subsection{Convolutional Neural Networks}
\textit{\glsreset{cnn}\glspl{cnn}} \cite{NIPS198953c3bce6, NIPS2012_c399862d} have emerged as a powerful class of deep learning models, particularly effective in computer vision tasks, including image and video analysis. The architecture consists of convolutional layers  implementing the fundamental convolution operation , pooling layers, and \glspl{dnn}. This architecture, presented in Figure~\ref{fig:cnnDesign}, enables \glspl{cnn} to automatically learn hierarchical representations of features while respecting properties of image-based data: spatiality (pixels have a position), locality (pixel share information within their neighbourhood), and symmetries.

\begin{figure}[h!]
    \center
    \includegraphics[width=0.9\textwidth]{Images/ML/cnn.png}
    \caption{A layer of a convolutional neural network, implementing a convolution with 4 kernels followed by a pooling operation. This design can be stacked to create deep architecture, and combined with a feed-forward neural network, after flattening the output at some depth, before reaching the final loss function layer.} 
    \label{fig:cnnDesign}
\end{figure}

\glspl{cnn} leverage convolutional layers to extract local patterns and features from input data. Convolutional operations are applied to the input data by multiplying, entry-by-entry, a learnable \textit{kernel} or \textit{filter}, represented by a matrix of weights of smaller dimension than the total image size, with an equal size subpart of the input and applying an activation function. The size of the kernel restricts the processing of the input to a given \textit{receptive field} dimension, and this window is passed over the full input image, moving it by a chosen \textit{stride} length. Pooling layers are then used to reduce spatial dimensions and retain important features. This process is parallelisable for an image with multiple channels. For classification and regression, a \gls{cnn} typically stacks several convolutional and pooling layers before leading to a fully connected neural network ``flattening'' the last representation to make predictions. \textit{Flattening} refers to the process of transforming the input image, represented as a matrix $\mathbb{R}^{x} \times \mathbb{R}^{y}$, into a vector $\mathbb{R}^{x \times y}$. \\

\gls{cnn}-based models, such as AlexNet \cite{NIPS2012_c399862d} and ResNet \cite{resNetPaper}, have demonstrated state-of-the-art performance in various computer vision tasks. A main advantage of the convolution operation on an image of size $x \times y$ is the reduction of the number of artificial neurons required to process the image, which helps to regularise the network. For a given image:
\begin{itemize}
    \item A \gls{dnn} processing the flattened image requires $x \times y$ neurons. 
    \item A \gls{cnn} with $k$ kernels of size $\alpha \times \beta$ requires $k \times \alpha \times \beta$ artificial neurons. 
\end{itemize}
For example, for an image of size 100 $\times$ 100, a \gls{dnn} requires 10,000 neurons while a \gls{cnn} can process the image with only 25 units if a single kernel of size 5 $\times$ 5 is used. Typical pooling functions are \textit{max pooling} or \textit{sum pooling}, which respectively take the largest element or the sum in each window of their input, with specific hyperparameters governing the size and the sliding of the window.

\begin{comment}
\paragraph{Pros:}
\begin{itemize}
    \item \textit{Feature Learning:} \glspl{cnn} automatically learn hierarchical representations of features on multidimensional data.
    \item \textit{Spatial Hierarchies:} convolutional and pooling layers enable the model to capture spatial hierarchies in the input data while respecting the properties of images. 
    \item \textit{State-of-the-Art Performance:} \glspl{cnn} have achieved state-of-the-art performance in image classification, object detection, and segmentation tasks.
\end{itemize}

\paragraph{Cons:}
\begin{itemize}
    \item \textit{Computational Complexity:} training deep \glspl{cnn} is computationally intensive.
    \item \textit{Large Datasets:} \gls{cnn} often require large datasets for effective training.
    \item \textit{Interpretability:} The internal workings of \glspl{cnn} are challenging to interpret.
\end{itemize}
\end{comment}

\subsection{Graph Neural Networks}\label{chapter-GNN}
Recently, \textit{\glspl{gnn}} have garnered attention for their ability to model and analyse complex relationships within graph-structured data \cite{graphNetRef}. Originally designed for tasks such as node classification and link prediction, \glspl{gnn} have found applications in diverse domains such as social networks modelling, recommendation systems, and physics, for modelling the dynamics of a $N$-body system, performing tracks reconstruction, and identifying particles. \\

\glspl{gnn} operate on graph-structured data, where nodes or vertices represent entities, and edges represent relationships between these entities. The functioning of \glspl{gnn} involves iterative aggregation of information from neighbouring nodes and updating of the edges, allowing them to capture both local and global patterns defined by the graph. An interesting feature of graphs is that the input information does not need to be given a rigid structure. Consequently, graph-based methods have a much greater representational power than image- or sequence-based ones. Graphs can represent arbitrary relational structures through directional weighted edges \cite{graphInductiveBias}. A particular feature arising from this property is that graphs are permutation equivariant: the order of nodes can be rearranged without consequence. \\

\begin{figure}[h!]
    \center
    \includegraphics[width=\textwidth]{Images/ML/gnn.png}
    \caption{The graph neural network update step of a directed graph $G(E, V, u)$ with a global representation $u$, four initial nodes $x_i \in V$ and edges $e_{ij} \in E$ connecting nodes $i \rightarrow j$. By analysing the neighbours of each node, the graph is updated to a new graph $G^*(E^*, V^*, u^*)$.} %(in the notation of the text, each node is given a different integer index $k$ and written $(e_k, r_k, s_k)$, with $r_k = j$ and $s_k = i$)
    \label{fig:gnnScheme}
\end{figure}

A \gls{gnn} architecture consists of multiple layers of message-passing operations. Each layer updates the node representations by aggregating information from neighbouring nodes, as schematised in Figure~\ref{fig:gnnScheme}. Different architectures implement different update processes, with popular \gls{gnn} architectures being \textit{Graph Convolutional Networks (GCNs)} \cite{gcnPaper} and \textit{\gls{gat}} \cite{velickovic2018graph}. In this thesis, the notation adopted is to represent a graph $G$ as a tuple of three sets $(E, V, u)$, where:
\begin{enumerate}
    \item $E = \{(e_k, r_k, s_k)\}_{k=1:N^e}$ is the set of edges, with each edge having a real vector of features $e_k \in \mathbb{R}^e$ and storing the index of the receiver (sender) as $r_k$ ($s_k$).
    \item $V = \{v\}_{i=1:N^v}$ is the set of nodes, each node having a real vector of features $v_i \in \mathbb{R}^v$.
    \item $u$ is a global attribute of the graph modelled by a real vector of features $u \in \mathbb{R}^u$. 
\end{enumerate}

The most general graph algorithm describing an update stage of a full \gls{gnn} block is presented in Algorithm \ref{algo:graph_network} of Appendix \ref{app:graph_update}. Essentially, for a given step the input is a graph $G(E, V, u)$ that is updated into a new graph $G^*(E^*, V^*, u^*)$ by first updating the edges $e \in E$, then modifying the nodes $v \in V$, and finally the global representation $u$. The update rule leverages different neural networks $\phi$ and aggregation functions $\rho$. The aggregation function must accept a variable number of inputs with permutation invariance to output a single element per group, and is typically implemented with the \textit{sum} or \textit{max pooling}. A global update is decomposed into the following successive steps: 
\begin{enumerate}
    \item Update the edges with a \gls{dnn} $\phi^e$ mapping each of the input edges, their respective receiver and sender nodes, and the global state $u$ to output a new edge feature vector $e^*_k$ for each edge $k$: $e^*_k = \phi^e(e_k, v_{r_k}, v_{s_k}, u)$. The new edges are stored in a set $E^*$. 
    \item Before updating a vertex $i$ represented by $v_i$, the $E_i^*$ updated edges connecting to $i$ are pooled locally over the node as $\overline{e^*_i} = \rho^{e \rightarrow v}(E_i^*)$.
    \item The vertex is then updated with a \gls{dnn} $\phi^v$ mapping the pooled representation of the edges $\overline{e^*_i}$ connected to the vertex being updated, the input vertex feature $v_i$, and the global representation $u$ to update $v_i \rightarrow v^*_i = \phi^v(\overline{e^*_i}, v_i, u)$. The new vertices are stored in $V^*$.
    \item The set of edges is updated through global pooling $\overline{e^*} = \rho^{e \rightarrow u}(E^*)$.
    \item The set of vertices is updated through global pooling $\overline{v^*} = \rho^{v \rightarrow u}(V^*)$.
    \item The global representation is updated by a \gls{dnn} $\phi^u$ mapping $u^* = \phi^u(\overline{e^*}, \overline{v^*}, u)$, with the globally pooled updated edges ($\overline{e^*}$) and vertices ($\overline{v^*}$). 
\end{enumerate} 
This formulation of a graph as a message-passing device with edge updates is the most complete architecture of a \gls{gnn}. However, the design is flexible: for example, \glspl{rnn} or \glspl{cnn} can be used instead of \glspl{dnn}. Furthermore, many specialisations of the structures exist to reduce the complexity of the model and avoid overfitting or convergence issues, as listed in Figure~\ref{fig:diverseGNN}. A notable example for this thesis is the \textit{Deep Set} architecture \cite{NIPS2017f22e4747}, designed to run specifically on sets where the ordering does not matter. It essentially simplifies the graph network by dropping altogether the edges and considering instead a fully connected graph with static edges, with an update of the global representation only based on pooled node information: 
\[v^*_i = \phi^v(\overline{e^*_i}, v_i, u) = \phi^v(v_i, u),\] 
\[\overline{V^*} = \rho^{v \rightarrow u} = \sum_i v^*_i,\] 
\[u^* = \phi^u(\overline{e^*}, \overline{v^*}, u) = \phi^u( \overline{v^*}, u).\] This is somewhat similar to \textit{PointNet}, a \gls{gnn} designed to analyse sets of 3D points, which uses an analogous update with max-aggregation instead of sum pooling after updating the nodes in two steps \cite{pointNet}.  

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[scale=0.43]{Images/ML/fullGNN.png}
        \caption{Full \gls{gnn}.} 
        \label{fig:diverseGNNfull}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[scale=0.43]{Images/ML/messagepassingNN.png}
        \caption{Message-passing \gls{gnn}.} 
        \label{fig:pullsFTAGmp}
    \end{subfigure}
    \\  % newline
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[scale=0.43]{Images/ML/deepSet.png}
        \caption{Deep Set.} 
        \label{fig:deepSetFig}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[scale=0.43]{Images/ML/nlnn.png}
        \caption{Non-local \gls{nn}.} 
        \label{fig:pullsFTAGnlnn}
    \end{subfigure}
    \caption{Different types of \gls{gnn} update rules, defining different \gls{gnn} architectures \cite{graphInductiveBias}.}
    \label{fig:diverseGNN}
\end{figure} 

A different approach introduced in \cite{nlnnPaper} defines the non-local neural network, unifying different types of \textit{attention}-based architecture. Attention is an essential feature of modern deep learning: it refers to a weighted version of the inputs, with weights representing the degree of attention given to different parts. Attention is not restricted to \glspl{gnn} but is easily encapsulated in their formalism. As will be shown in the next section, the transformer is a special case of a non-local \gls{nn}. In this section, only \textit{\glsreset{gat}\glspl{gat}} are introduced for conciseness \cite{velickovic2018graph}. \glspl{gat} implement a learnable weighting of the neighbours of the node being updated. When updating node $v_i$, a score is computed for each of the connected neighbours $v_j$ of $v_i$ by a \gls{nn} mapping \[e(v_i, v_j) = \phi(v_i, v_j) = a^T \text{ leakyReLU}([W v_i, Wv_j]),\] where the nodes $v_i, v_j \in \mathbb{R}^d$ are connected and the operation implements an embedding of the two nodes to a dimension $d'$ with two learnable parameters: $a \in \mathbb{R}^{2d'}$ and $W \in \mathbb{R}^{d' \times d}$. The operator $[,]$ stands for matrix concatenation. These scores are then combined for each node $i$ over its neighbours $\{j\}$ to give $\alpha_{ij}$ attention scores \[ \alpha_{ij} = \text{softmax}_j (e(v_i, v_j)) = \frac{\exp(e(v_i, v_j))}{\sum_{j' \in \text{neighours of i}}e(v_i, v_{j'})}.\] The final step is to leverage these attention weights when updating each node $v_i$ as \[v^*_i = \sigma\left(\sum_{j} \alpha_{ij} W^v v_j \right),\] where the sum over $j$ is taken over neighbouring nodes of $i$, $\sigma$ is an activation function, and $W^v$ is another matrix of learnable parameters.

\begin{comment}
\paragraph{Pros:}
\begin{itemize}
    \item \textit{Modeling Graph Structure:} \glspl{gnn} naturally handle graph-structured data, making them well-suited for tasks involving abritrary relationships between entities.
    \item \textit{State-of-the-Art Performance:} \glspl{gnn} have achieved state-of-the-art results in various graph-related tasks.
\end{itemize}

\paragraph{Cons:}
\begin{itemize}
    \item \textit{Computational Complexity:} training \glspl{gnn} can be computationally expensive.
    \item \textit{Limited Global Context:} some \gls{gnn} architectures may struggle to capture long-range dependencies in graphs, limiting their ability to consider global context.
    \item \textit{Interpretability:} \glspl{gnn} are not directly interpretable.
\end{itemize}
\end{comment}

\subsection{The Rise of the Transformers}\label{sec:transformer}
The \textit{transformer} architecture, introduced in 2017 \cite{NIPS_transformerPaper}, has become a foundational and ubiquitous design across machine learning. It has significantly impacted the field, enabling the development of state-of-the-art models such as BERT \cite{devlin-etal-2019-bert} and GPT \cite{radford2018improving}. More recently, the transformer has also spearheaded a revolution in computer vision tasks thanks to the generalisation of the architecture into the \textit{Vision Transformer (ViT)} \cite{vitPaper}.\\

Transformers rely on the mechanism of self-attention, as introduced in the previous section. It removes the sequential processing of \glspl{rnn} by favouring a fully parallelisable approach, allowing for efficient computation on dedicated hardware. The key components in the transformer are the self-attention mechanism and position-wise feed-forward networks. Self-attention allows the model to weigh the importance of different words or tokens in a sequence for each token. This mechanism enables the model to capture long-range dependencies in the input data without the added complexity of \gls{lstm}. Strictly speaking, the input to a transformer is an unordered sequence. For \gls{nlp}, the ordering is built into the model with positional encoding, giving the model a handle to determine the index of the token in the sequence. In computer vision, the vision transformer first splits the input image $x$ into patches of fixed size, flattens them into vectors, and maps them with a learnable positional encoding before processing them as a classical transformer.

\begin{figure}[h!]
    \center
    \includegraphics[width=\textwidth]{Images/ML/transformer.png}
    \caption{The full transformer architecture, combining an encoder and a decoder, each made of an arbitrary number of layers. The input $x$ is first embedded with a dedicated mapping and, when order matters, supplemented with positional encoding. The encoder generates an internal representation $h$ that is passed to the decoder. This component, depicted on the right, produces the next output using the internal representation $y$ and the output shifted to the right to force output tokens to access only prior information.} 
    \label{fig:tranfoArchi}
\end{figure}

The general transformer architecture, presented in Figure~\ref{fig:tranfoArchi}, consists of an encoder and a decoder. The decoder works in an autoregressive way, combining the current outputs $y_{t<T}$ with an internal representation $h$ built by the encoder to generate the next output tokens $y_{T}$. Both the encoder and the decoder are composed of multiple layers, each containing a multi-head self-attention mechanism, feed-forward networks (\glspl{dnn}), and optionally positional encodings. The decoder is further endowed with a masked attention layer, for the output to compute self-attention with information accessible only before the token's position. The attention mechanism allows the model to focus on different parts of the input sequence, while the feed-forward networks provide additional non-linear transformations. Residual connections are added to let the gradients propagate efficiently in-depth, and layer normalisation is used after each block to avoid vanishing or exploding gradients and improve training speed \cite{ba2016layer}. This type of normalisation scales each activation (each neuron) by subtracting the empirical mean and dividing by the standard deviation per data point. \\

The attention mechanism maps the queries and a set of key-value pairs to an output as defined in Equation \ref{eq:scdotatt} and schematised in Figure~\ref{fig:scaledDotAtt}, with query $Q \in \mathbb{R}^{d_k \times d_q}$, key $K \in \mathbb{R}^{d_k \times d_v}$, and value $V \in \mathbb{R}^{d_v}$ and the output is a vector $\mathbb{R}^{d_q \times d_v}$ of values reweighted by the attention scores. This combines $d_q$ different queries of the $d_k$ keys mapping to $d_v$ values. Therefore, Equation \ref{eq:scdotatt} implements for each query a weighted sum of the values, based on a compatibility function established by comparing the queries and keys:
\begin{equation}\label{eq:scdotatt}
    \text{Attention}(Q, K, V) = \text{softmax}\left( \frac{Q^T \, K}{\sqrt{d_k}}\right) V,
\end{equation} 
where the scaling $\sqrt{d_k}$ is applied per row of the attention matrix to reduce the magnitude of the dot product $Q^T K$, preventing saturation in the subsequent softmax. This scaled dot-product attention mechanism leverages the extensive research into numerical optimisation of matrix multiplications, making this operation less time and memory-consuming than using a \gls{dnn} mapping to compute the attention - a technique referred to as \textit{additive attention} \cite{Bahdanau2014NeuralMT}. As shown in Figure~\ref{fig:mulitheadAtt}, multi-head attention runs this dot-product attention in parallel for $h$ different heads, each head $h_i$ ($i = 1, ..., h$) implementing a separate learnable projection from the input $(Q, K, V)$ with linear transformations of weights $W_i^Q \in \mathbb{R}^{d \times d_k}$, $W_i^K \in \mathbb{R}^{d \times d_k}$, and $W_i^V \in \mathbb{R}^{d \times d_v}$, where $N$ is the length of the sequence, $d$ is the model dimension: \[Q_i = Q W_i^Q, \quad K_i = K W_i^K, \quad V_i = V W_i^V,\] \[H_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V).\] The multi-head module then concatenates the $h$ different heads $H_i$ and applies another linear transformation with parameters $W^O \in \mathbb{R}^{hd_v \times d}$ to generate the output \[\text{Multi-Head Attention}(Q, K, V) = \left[H_1, ..., H_h\right]W^O .\] In multi-head attention, there are therefore 3 different learnable projections $W_i^Q$, $W_i^K$, and $W_i^V$ per head and a single global projection $W^O$ for the output of the cell. Self-attention is a special case in which the input of the cell is not a tuple $(Q, K, V)$ of distinct vectors but a single input $x \in \mathbb{R}^{N \times d_v}$ that is mapped to the tuple with the learnable projections: \[Q = xW_i^Q, \quad K = xW_i^K, \quad V = xW_i^V.\]

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[scale=0.65]{Images/ML/scaledDotAtt.png}
        \caption{Scaled dot-product attention with optional masking.} 
        \label{fig:scaledDotAtt}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[scale=0.65]{Images/ML/multiHeadAtt.png}
        \caption{Multi-head attention module, where $L$ stands for a linear transformation of the input.} 
        \label{fig:mulitheadAtt}
    \end{subfigure}
    \caption{Multi-head attention mechanism in a transformer. The core operation is an optionally masked scaled dot-product attention of the queries $Q$, keys $K$, and values $V$. A head receives linear projections of the tuple $(Q, K, V)$, each implementing a separate scaled dot-product. The multi-head modules then concatenate all the different head results and finish with another linear transformation.}
    \label{fig:transAtt}
\end{figure} 

\begin{comment}
\paragraph{Pros:}
\begin{itemize}
    \item \textit{Versatility:} the transformer architecture has been successfully applied to various \gls{nlp} and computer vision tasks.
    \item \textit{Parallelisation:} the architecture is efficiently parallelised, speeding up the computations.
    \item \textit{Capturing Dependencies:} the self-attention mechanism enables the model to capture complex long-range dependencies.
    \item \textit{Stability:} transformers benefit from inbuilt regularisation effects due to residual connections and normalisation layers. Models based on this structure can therefore simply scale in complexity and resist well to overtraining.
\end{itemize}

\paragraph{Cons:}
\begin{itemize}
    \item \textit{Computational Complexity:} large transformer are computationally expensive to train.
    \item \textit{Interpretability:} the attention mechanism is challenging to interpret.
    \item \textit{Data Dependency:} transformer models require large amounts of data for effective training, limiting their application in domains with limited statistics.
\end{itemize}
\end{comment}

\section{Training and Optimising Deep Learning Models}
Training and optimising neural networks involve a combination of selecting appropriate architectures, fine-tuning the hyperparameters that are not learnt by backpropagation, and employing acceleration techniques to improve efficiency and convergence. In this section, key aspects of the training process are explored.

\subsection{Training Algorithms}
When optimising the learnable parameters of a model, different training algorithms can be deployed to update the weights. All strategies are refinements of the gradient descent rule of Equation \ref{eq:gradientdescent}, and each method has different advantages. The two main approaches are: 
\begin{itemize}[leftmargin=*]
    \item \textit{\gls{sgd}}: instead of deriving the gradients on the whole dataset (\textit{full-batch}), the expected gradient over a random sub-batch of $b$ elements is taken \[ \nabla w_b = \frac{1}{b} \sum_{s=1}^b \nabla w_s,\] with $\nabla w_s$ the gradient of the parameter $w$ computed for a single data point $s$. A common observation is that for sub-batches - from now on referred to as just \textit{batches} - of sufficient size $b$, the statistical estimator of the gradient based on the batch is unbiased. This greatly reduces the time needed to compute the gradient and naturally splits the loop over the dataset into different iterations called \textit{steps} at which a batch is passed through the network. This is a beneficial feature when dealing with large datasets that would not fit in memory. \gls{sgd} has also a positive effect on the regularisation of the model, as each per-batch gradient has a larger variance than a full-batch, making it harder for the model to overfit, but slowing the convergence to an optimum. 
    \item \textit{Adam} is an algorithm published in 2014 leveraging an adaptive moment estimation approach as well as batch processing \cite{adamPaper}. The moment in this sense is analogous to the physical moment and encapsulates the dynamic of the optimisation as driven by gradient descent. The fundamental idea is that larger gradients indicate a steeper slope that can be quickly traversed so that any slowing down due to a changing curvature of the objective function landscape can be mitigated from the inertia of the previous gradients. This behaviour is implemented as an exponentially decaying moving average: the moment $m^t_w$ of weight $w$ at step $t$ is updated with a gradient forgetting factor $\beta_1 \in [0, 1[$ such that \[ m_t \leftarrow \beta_1 m^{t-1}_w + (1 - \beta_1) \nabla w_b^t,\] where the previous contribution is successively multiplied by $\beta_1$, reducing the importance of earlier gradients progressively. Additionally, another element is taken into account in the gradient descent rule: the second moment $(\nabla w_b^t)^2$. This tracks the magnitude of the gradient and, by multiplying the gradient update by a term inversely proportional to the second moment, accelerates the gradient updates in flatter regions of the objective landscape, which have small gradient magnitudes, with the term \[ v_t \leftarrow \beta_2 v^{t-1}_w + (1 - \beta_2) (\nabla w_b^t)^2,\] where a second moment forgetting factor $\beta_2 \in [0, 1[$ is introduced. To avoid biasing the gradient update, both the first and second moments are corrected with \[\hat{m}^t_w \leftarrow \frac{m^t_w}{1 - \beta_1} \quad \text{ and } \quad  \hat{v}^t_w \leftarrow \frac{v^t_w}{1 - \beta_2}.\] The two contributions are then combined into a single gradient descent step as
    \begin{equation}\label{eq:adam}
        w^{t} \leftarrow w^{t-1} - LR \times \frac{\hat{m}^t_w}{\sqrt{\hat{v}^t_w} + \epsilon},
    \end{equation}
    where a very small $\epsilon$ is added for numerical stability.
\end{itemize}

A key hyperparameter in any gradient descent-based algorithm is the learning rate $LR$. There is no evident choice for this parameter, and suitable values have to be derived on a case-by-case basis. A useful technique to allow the training process to converge to a good minimum of the loss function and avoid unsuitable local minima is to adopt a \textit{learning rate schedule}, modifying the parameter throughout training to navigate different parts of the loss function landscape. Initially, a relatively large $LR$ allows the model to quickly update its weights in the direction of the minimum. If the rate is kept too high, the weights will not approach the minimum and will overshoot or ``bounce'' around the optimum. To avoid this, the scheduler reduces the learning rate to later take smaller steps and converge towards the optimum. Initially, the rate is typically not set to its maximum value to guide the gradient updates to a valley of interest. An equivalent choice is to modify the batch size while keeping the $LR$ fixed \cite{smith2017decay}. This also has a regularising effect on the gradient: small batch sizes capture large variances and allow the optimisation to make drastic changes in orientation in the optimisation function landscape, thereby avoiding unsatisfactory local minima. Having larger batch sizes stabilises the direction of descent, thereby offering lower variance but potentially biasing estimates towards a minimum. Combining these two characteristics at different epochs of the training is an effective way to improve training performance. Some methods, such as Adam, have additional hyperparameters to optimise.

\subsection{Regularisation}
Regularisation techniques are applied in the architecture and training procedure to prevent overfitting. Common methods include \textit{dropout}, which randomly drops connections or neurons with probability $p$ during training. \textit{Regularisers} can be added to the loss function to restrict the size of the weights: either with the L2-penalty or L1-penalty, respectively penalising the sum of the squared or absolute values of the weights, as \[\lambda \sum_i w_i^2 \quad \text{ or } \lambda \sum_i |w_i|.\]. The latter approach results in sparse networks where unnecessary weights are set to 0. The amount of regularisation is controlled by the regularisation hyperparameter $\lambda$. Both $p$ and $\lambda$ require careful optimisation as regularising the model can introduce bias and affect overall performance. Additionally, batch normalisation is a technique that normalises the inputs of a layer across the batch, reducing internal covariate shifts. This helps to stabilise and accelerate the training process. This is distinct from the layer normalisation used in the transformer architecture, as the normalisation is applied across the batch samples rather than the activations. 

\subsection{Architecture \& Hyperparameters Optimisation}
Several characteristics of the network need to be optimised: 
\begin{itemize}[leftmargin=*]
    \item \textit{Architecture Selection:} choosing the right architecture is crucial to get the best performance. Factors to consider include the complexity of the task, the nature of the data, and the desired trade-off between model complexity and interpretability. Limits in computing power should be factored in. Elements of the architecture include the type of \gls{ml} chosen (\gls{bdt}, \gls{dnn}, transformer, ...), the choice of activation functions (\gls{relu}, tanh, ...), and the number of layers, nodes, and connections between the units.
    \item \textit{Hyperparameter Tuning:} optimising the hyperparameters - parameters that are not optimised through backpropagation of the loss - is essential for achieving the best possible performance. Key hyperparameters include optimiser-related parameters such as the learning rate, batch size, regularisation parameters, and initialisation of the weights and biases.
\end{itemize}
The optimisation process for both hyperparameter tuning and architecture selection is expensive: the model must be trained with different combinations of hyperparameters and architecture to evaluate their respective performance and uncover the best-performing options. Techniques such as grid search, random search, and Bayesian optimisation can be employed to efficiently explore the hyperparameter space. Architecture search is usually performed by trial and error, with the \gls{ml} literature offering some insight into what models might best perform in specific situations. 

\subsection{Acceleration Techniques}
Training an \gls{ml} model is a computationally demanding task that can be carried out more effectively on specifically designed hardware and with some tricks in the process. 
\begin{itemize}[leftmargin=*]
    \item \textit{Parallel Dataloading:} can significantly speed up the training process. Instead of preparing a single batch, multiple batches can be loaded by different processing units in parallel to avoid this bottleneck. 
    \item \textit{Early Stopping:} prevents overfitting and saves computation time by interrupting the training when the performance saturates.
    \item  \textit{Hardware Accelerators:} specialised hardware accelerators can accelerate the training. \textit{\glsreset{gpu}\glspl{gpu}} are specifically designed to perform matrix operations in parallel, and are therefore well suited for \gls{ml}. Utilising \glspl{gpu} for training and inference can give substantial speedups compared to \gls{cpu}-based computing. 
    \item \textit{Transfer Learning:} leverages pre-trained models on large datasets and applies them to a new task. The idea is that there are some fundamental similarities between the new task and the task used to pre-train the model, giving the latter a head start. Fine-tuning a well pre-trained model on specific tasks or connecting additional modules can significantly reduce the required data and training time compared to training from scratch. This approach is becoming increasingly popular, as large \textit{foundation} models trained on multiple tasks with huge datasets can then be applied to a specific task, with the pre-trained weights either kept fixed or modified for the new task. Such large foundation models are already available in \gls{nlp} (e.g., the 7 billion parameters Mistral transformer \cite{jiang2023mistral}) and computer vision (e.g., the 500 million parameters Florence-2 \cite{xiao2023florence2}).
\end{itemize}
Training and optimising deep learning models involve a combination of careful architectural choices, hyperparameters tuning, and the use of acceleration techniques. Selecting the most appropriate techniques is a task-specific challenge that depends on available resources and trade-offs between training time, model performance, and interpretability.
