\subsection{GN2 Hyperparameters Optimisation}\label{chap-GN2Opt}
The state-of-the-art flavour tagger at ATLAS is, at the time of writing, built on the \gls{gn2} architecture. Naturally, fine-tuning the model is required to further push the performance higher. Many studies are ongoing to deliver yet a stronger tagger than the \gls{gn2} version presented in this thesis. A non-exhaustive lists of ongoing research directions include: 
\begin{itemize}
  \item Optimising the track selection and the jet reconstruction type. Moving towards yet a looser selection and letting the network sift through a larger set of background tracks could deliver further performance. Assessing the effect of modelling uncertainties is however of particular importance for these modifications.
  \item The inclusion of neutral constituent information by using jets defined as \gls{ufo}. Tracks are reconstructed from hits in semiconductor-based detectors. Such hits are only recorded for charged particles flying through the active regions of the sensors. This approach entirely misses neutral particles, such as neutrons, neutral pions and kaons, and neutrinos. All but the latters leave energy in the calorimeters that is measurable and accessible. The \gls{ufo} jet definition combines track information with calorimeter topoclusters objects. Studies are ongoing to add this information to the set of tracks. 
  \item The inclusion of leptonic information. 40\% of $b$-hadrons include either an $e$ or a $\mu$ in the jet cone \cite{Tanabashi:2018oca}. As seen with \gls{gn1}, the inclusion of leptonic information in the set of tracks leads to a significant performance increase. Studies are ongoing to build a finer lepton-information analyser within \gls{gn2}.
  \item Hadronic decays of $\tau$ are a major source of background for analyses focusing on $c$-jet tagging, due to their similar signatures. Including these leptons in the classification objective has been seen to deliver promising results in initial studies. 
  \item Finer output classes categorisation. Currently, the simple labelling scheme deployed combines topologies with significant differences. For example, purely hadronic and semi-leptonic decays of $b$-jets are both labelled $b$-jets. Adopting a greater flexibility in the definition of classes allows the model to fully utilise the unique signature of each process. 
  \item Integrating further expert information into the design is known to deliver a great boost to performance. Studies are ongoing to upgrade the set of auxiliary tasks, in particular for secondary vertex fitting and reconstruction. A \gls{gn2} model able to reliably reconstruct this information would have a use case in the ATLAS experiment beyond heavy-flavour jet tagging, while benefitting from improved performance for this essential task.  
\end{itemize}

These design considerations are paramount to produce a more efficient tagger. An equally essential endeavour is to fine-tune the architecture to extract the best performance from a chosen strategy. This section focuses on some initial studies to perform \gls{hpo} and network architecture search for \gls{gn2}. The essential challenge is that a test of a change to the hyperparameters or to the model architecture requires to fully retrain a \gls{gn2} model from scractch. This is a costly process, as a single epoch of \gls{gn2} training takes roughly $\sim$28 min for 2 NVIDIA A100 \glspl{gpu} each fed data by 20 \gls{cpu} on a 30 million jets dataset with batchsize 2000 evenly split on the \glspl{gpu}. \gls{gn2} has many hyperparameters that should be optimised to deliver optimal performance, among which the most relevant are: initial $lr$, maximal $lr$, end $lr$, the weights of the 2 auxiliary tasks, the amount of weight decay, the batchsize, and the floating numbers precision. Important architecture-level elements to be optimised are the embedding dimension (output of the initialiser and as input and output of each transformer encoder), the depth of the initialiser, the number of layers and heads in the transformer encoder, the size of the transformer output, the auxiliary tasks \gls{dnn}, the activation functions, and the specific loss functions and their class-weights used. \\

Unfortunately, access to \glspl{gpu} is, at the time of writing, limited for members of the Collaboration. Most of the computing power leveraged to train advanced \gls{ml} models such as \gls{gn2} is accessed on high-performance cluster of institutes to which members belong. In this respect, a promising area of development is being pursued by \gls{cern}, with the introduction of a KubeFlow-backed served hosted on \textit{ml.cern.ch} \cite{KubeFlowCern}. KubeFlow was created by Google and is now backed by the Cloud Native Computing Foundation. It is an open-source framework built on Kubernetes to perform machine learning operations such as training, inference, deployment, and hyperparameter optimisation. The project aims to centralise some \gls{gpu} resources into a single cluster with datastorage, efficient I/O reading capabilities, and dedicated \gls{gpu} nodes. Katib, KubeFlow's dedicated \gls{hpo} workload, is a promising approach to perform effective hyperparameter optimisation with state-of-the-art autoML techniques that automate and refine the strategy to test and converge on the best hyperparameters \cite{george2020katib}. At the time of writing, the server is still in a beta phase with little hardware accessible, thereby removing it from consideration as a possible solution to carry out the full \gls{hpo} of \gls{gn2}. However, the \textsc{salt} framework used to train \gls{gn2} was adapted to run on any KubeFlow platform, with initial tests showed promising possibilites for the Collaboration. Being accessible to any member of ATLAS, this project would ``democratise'' access to computing intensive studies for institutes lacking an advanced \gls{hpc}. \\

Large \gls{nn} such as large language models that are being developped at ATLAS will require clusters designed for machine learning, with many \glspl{gpu} accessible on dedicated nodes. This paradigm of computing is markedly different from the typical grid-base distributed computing deployed in particle physics experiments. While \gls{mc}-based samples and sub-sampled datasets can be effectivly processed by autonomous parallel jobs, \gls{ml} requires communication between the different jobs to keep the weights of the model trained synchronised on the different \glspl{gpu}. A fast connexion between these \glspl{gpu} is essential, as is having fast read access to the full dataset due to the need to loop over the whole data for each epoch during training. Distributing the computation across different \gls{hpc} that are geographically distant, as is common with the current \gls{cern} computing grid, is not effective for this purpose. The \gls{cern} KubeFlow server is a promising area of development for the future computational needs of ATLAS. Furthermore, having a framework compatible with KubeFlow allows operating on multiple platforms, giving the flexibility to scale resource access for computationally demanding tasks, such as \gls{hpo}. Most private and public cloud providers, such as Google Cloud, Amazon Web Service, and Microsoft's Azure, are KubeFlow-compatible and host a larger amount of state-of-the-art \glspl{gpu}. \textsc{Salt} can be effectivly deployed on one of these cloud providers infrastructure or on the \gls{cern}'s KubeFlow server with no noticeable distinctions for the user. \\

While leveraging a large amount of computing power is a natural solution to the challenging task of \gls{hpo} of a ``large'' neural network by ATLAS standards, a more refined technique can be exploited in the present case. Recent works from the \gls{ml} community suggest that the optimal hyperparameters of a nominal model can be estimated from a smaller model \cite{yang2021tuning}. Here smaller refers to either the depth - the number of layers - or the width - the number of neurons per layer and, in the case of a transformer, also the number of heads in the multihead attention - of the neural network. Ref. \cite{pmlr-v139-yang21c} establishes the mathematical foundation backing this surprising behaviour of deep neural network: the \gls{mup}. The rest of this section is dedicated to introducing and defining the maximal update parametrisation before establishing its relevance for \gls{hpo}. \\

\subsubsection{Maximal Update Parametrisation}
The maximal update parametrisation is first and foremost a \textit{parametrisation}. In this context, the parametrisation of a neural network refers to the definition of the weights of each individual neurons, the way they are initialised, and how they are updated from a given optimisation algorithms, such as Adam or \gls{sgd} \cite{adamPaper}. The default or \textit{standard} parametrisation (SP) follows the so-called LeCun parametrisation \cite{LeCun2012}. This parametrisation, routinely deployed in \gls{ml} frameworks such as PyTorch \cite{pytorch}, initialises the weights by sampling them from a Gaussian or Uniform distribution with mean 0 and standard deviation given bt the inverse of the input dimension of the layer the weight belongs to. For both Adam and \gls{sgd}, a single master learning rate (LR) $\eta$ is used for all weights. For \gls{mup}, some subtle differences are introcuded, as summarised in Table~\ref{tab:mupvsspdef}. Mainly, the output layer weights are sampled from a Gaussian with a standard deviation being the inverse of the input dimension \textbf{squared} of the output layer. Concerning the learning rates, the hidden and output layers are scaled down by their respective input dimension for Adam. For \gls{sgd}, the output layer LR is scaled similarly, but the input and the bias LR are scaled up by the output dimension of these layers. 

\begin{table}[h]
  \begin{center}
      \begin{tabular}{c|cc|cc|cc} 
      	 \hline \hline
          & \multicolumn{2}{c|}{Initialisation Distribution} & \multicolumn{2}{c|}{Adam LR} & \multicolumn{2}{c}{SGD LR}  \\
            & $SP$  & \gls{mup} & $SP$  & \gls{mup} & $SP$  & \gls{mup}  \\ \hline
          $w^{L_\textrm{inp}}$ & $\sim$ $\mathcal{N}\left(0, \frac{1}{d^{\textrm{in}}_{L_\textrm{inp}}}\right)$ & $\sim$ $\mathcal{N}\left(0, \frac{1}{d^{\textrm{in}}_{L_\textrm{inp}}}\right)$                                         & $\eta$ & $\eta$                                    & $\eta$ & $\eta \times d^{\textrm{out}}_{L_\textrm{inp}}$ \\ 
          $w^{L_\textrm{hid}}$ & $\sim$ $\mathcal{N}\left(0, \frac{1}{d^{\textrm{in}}_{L_\textrm{hid}}}\right)$ & $\sim$ $\mathcal{N}\left(0, \frac{1}{d^{\textrm{in}}_{L_\textrm{hid}}}\right)$                                         & $\eta$ & $\eta / d^{\textrm{in}}_{L_\textrm{hid}}$ & $\eta$ & $\eta$                                    \\ 
          $w^{L_\textrm{out}}$ & $\sim$ $\mathcal{N}\left(0, \frac{1}{d^{\textrm{in}}_{L_\textrm{out}}}\right)$ & $\sim$ $\mathcal{N}\left(0, \frac{1}{d^{\textrm{in}}_{L_\textrm{out}}\times d^{\textrm{in}}_{L_\textrm{out}}}\right)$  & $\eta$ & $\eta / d^{\textrm{in}}_{L_\textrm{out}}$ & $\eta$ & $\eta / d^{\textrm{in}}_{L_\textrm{out}}$ \\
          $b^{L} \;\forall L$ & 0 & 0                                                                                                                                                                                          & $\eta$ & $\eta$                                    & $\eta$ & $\eta \times d^{\textrm{out}}_{L}$              \\  \hline \hline
      \end{tabular}
    \caption{Comparing the Standard Parametrisation (SP) to the Maximal Update Parametrisation ($\mu P$), as defined in Ref. \cite{yang2021tuning} based on the work of Ref. \cite{pmlr-v139-yang21c}.}
    \label{tab:mupvsspdef}
  \end{center}
\end{table}

This particular derivation of \gls{mup}, taken from Ref. \cite{yang2021tuning}, is equivalent to the original \gls{mup} derivation introduced in Ref. \cite{pmlr-v139-yang21c}. \gls{mup} turns out to be the unique parametrisation that maximally updates the weights of a neural network. The updates are ``\textit{maximal}'' in the sense that they are as large as they could be for a given LR to avoid any instabilities. For the specific case of the attention mechanism computed by the multihead attention of transformers, the scaling has to be modified from $\sqrt{d_k} \rightarrow d_k$ to properly scale with width \cite{yang2021tuning}. Figure~\ref{fig:muspweights} shows a comparison of the size of the pre-activation of a \gls{gn2} model with \gls{mup} parametrisation to a standardly parametrised \gls{gn2}, referred to as the $SP$ model, at different trainign steps. Each curve displays, for different embedding width in the transformer and the track initialiser, the sum of the absolute values of the weights before the activation ($L_1(\textrm{layer}) = \sum_{w_i \in \textrm{layer}} |w_i|$) for the initialiser and transformer models only. Three timesteps are displayed for each model, the initialisation ($t=1$) and after 1 ($t=2$) and 2 ($t=2$) training steps. The interesting behaviour highlighted in this figure is that for the $SP$ model, the pre-activation weights blow up with width during training as shown by the exponential rise of the sum of pre-activations. For \gls{mup} however, the $L_1$ of each layer stays flat with width even during training, proving the correct parametrisation of the model and the ``width-independent'' scaling. This unstable behaviour of the $SP$ parametrisation is easily highlighted thanks to the use of a large and fixed learning rate (here $lr = 10^{-2}$). 

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{Images/FTAG/GN/HPO/spweights.png}\\
  \includegraphics[width=\textwidth]{Images/FTAG/GN/HPO/mupweights.png}
  \caption{The sum of the absolute value of the pre-activation weights for the different layers in the initialiser and transformer parts of a GN2-like model in standard parametrisation ($SP$ - top) and in $\mu$P parametrisation (bottom), at three timesteps: initialisation ($t=1$ - left), after one training step with $lr = 10^{-2}$ ($t=2$ - centre), and a second training step ($t=3$) \cite{publicplotMUP}. The models displayed are labelled GN2-like as they lack auxiliary tasks.}
  \label{fig:muspweights}
\end{figure} 

Theoretically, a \gls{mup} model should deliver equal to better performance to an equivalent $SP$ model when both have optimal hyperparameters. This behaviour is due to the maximal updating of the former, leading to optimal in-depth updates of all layers. The standard parametrisation does not implement this correct updating, with outter layers closer to the loss function having an opacity effect on the propagation of the update for the input layers proportionally to their widths. Scaling down the learning rate is not a sufficient modification to correct the SP: as displayed in Figure~\ref{fig:muspweights}, not all layers update incorrectly with some pre-activation sum staying flat across width. By updating all activation maximally independently of the width, \gls{mup} outperforms $SP$ for a tuned learning rate \cite{pmlr-v139-yang21c}. A significant advantage of this parametrisation is that the optimal learning rate for a \gls{mup} architecture becomes width-independent. This leads to the $\mu$Transfer algorithm for \gls{hpo}, where the best hyperparameters for a \gls{mup} model are found on a version with fewer neurons per layers (smaller width) and the found optimal ones are transferred to the full-size model at no extra cost (0-shot transferred) \cite{yang2021tuning}. The benefits of adopting the maximal update parametrisation are:
\begin{enumerate}
  \item Better performance of a \gls{mup} model compared to an $SP$ model for a tuned learning rate.
  \item Improved hyperparameter optimisation with the $\mu$Transfer algorithm: performing the \gls{hpo} scan on a smaller and easier to train model to 0-shot transfer the best set of hyperparameters to the full-size models. 
  \item Better hardware usage for \gls{hpo}: a smaller model can be trained on a single \gls{gpu}. This is of particular interest for the ATLAS Collaboration, as most of the \gls{gpu} resources accessible are scattered through geographically distant computing sites.  
  \item Simplified architecture: with \gls{mup}, a wider model outperforms a smaller model if no overtraining occurs. Therefore, the best learning rate hyperparameter has to be found ounce for all \gls{gn2} model of varying widths and the widths are chosen based on the desired computational complexity.
\end{enumerate}

Hyperparameters that can be optimised with the $\mu$Transfer algorithms are said to be $\mu$Transferable. They consist of \cite{yang2021tuning}: 
\begin{itemize}
  \item Learning rate and parameters of a learning rate scheduler.
  \item Optimiser parameters, such as the momentum, and the Adam $\alpha$ and $\beta$.
  \item Initialisation parameters, such as the initial per-layer variances.
  \item Multiplicative constants.
\end{itemize}
Unfortunately, many parameters do not $\mu$Tranfer as they combine aspects of the model and the data, and must be studied on the full size model directly. For example, the regularisation parameters (dropout, weight decay, normalisation, ...) do not scale, as a particular model size will overfit depending on the data. Finally, the last important family of hyperparameters are those defining the scale of the problem. These parameters are not found from $\mu$Transfer but rather ``$\mu$Transfered along''. They consist of the width\footnote{Number of neurons per layer, number of attention heads in a transformer, ...}, the depth, and the batchsize. Only the scaling along width is theoretically proven thanks to \gls{mup}, while the others are empirically observed to hold \cite{yang2021tuning}.\\

Studies of the \gls{mup} parametrisation and the $\mu$Transfer algorithm have been performed for the \gls{gn2} flavour tagger. In this architecture, the most relevant dimensions are the width and the depth of the transformer part, tasks with building a conditional representation of the tracks from the embedded tracks processed by the initialiser network. These two dimensions are keys as most of the parameters of the \gls{gn2} model are in the transformer and the initialiser, with only few parameters set in the networks of the primary and auxiliary tasks. As such, the chosen dimension to scale with $\mu$Transfer is the embedding width. The number of parameters in the tranformer associated to the embedding width scales with this parameter, making it the most sensitive dimension to define the complexity of \gls{gn2}. 

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{Images/FTAG/GN/HPO/maincompmupsp.png}
  \caption{Comparison of a maximal learning rate value scan at an initial learning rate value of $10^{-5}$ for an $SP$ (left) and a \gls{mup} GN2 models (right) for three different embedding widths: 64 (yellow), 128 (red), and 256 (purple). The $y$-axis displays the validation loss attained. Taken from \cite{publicplotMUP}.}
  \label{fig:maincompmupsp}
\end{figure} 

To demonstrate the effect of \gls{mup} on \gls{gn2}, a learning rate hyperparameters optimisation campaign targeting the initial and maximal value of the learning rate\footnote{The final value - LR end - was not modified and is kept at $10^{-5}$ for all test due to limited compute.} is performed using the standard and maximal update parameterisation ($SP$ vs \gls{mup}). Three embedding widths are considered: the nominal 256 embedding width, defining a \gls{gn2} model with 2.3M parameters, a mid-size 128 embedding width (0.72M parameters), and a small 64 embedding width model with 0.23M parameters. Interestingly, this smaller model with an embedding 1/4 of the full model only has a 10th of the parameters. Furthermore, the small model was found to be trainable on a single \gls{gpu} while the full and mid-size models required two \glspl{gpu} to be trained in a reasonable amount of time. All models are full \gls{gn2} models trained on 30M PFlow jets\footnote{Composed of 60\% $t\bar{t}$ and 40\% $Z'$.} for 40 epochs with batchsize 1024. Parameters not mentioned are kept similar between embedding widths and parametrisation, and the epoch giving the lowest validation loss is chosen for each run. Figure~\ref{fig:maincompmupsp} displays the main result from this campaign, displaying the various LR max considered at the best LR initial found ($10^{-5}$). Three main observations are drawn from analysing the result:
\begin{enumerate}
  \item With \gls{mup}, the wider \gls{gn2} models - larger embedding width - always outperform the smaller versions. 
  \item Wider models do not always outperform smaller model with SP. In particular, at large LR max, the wider model becomes unstable and its performance in terms of validation loss significantly decreases.
  \item The optimal LR max (and LR init as shown in Figure~\ref{fig:fullSPmup}) are shared across width with \gls{mup}, while no such behaviour is guaranteed for $SP$ - but is observed in the present case.
\end{enumerate}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{Images/FTAG/GN/HPO/fullSP2.png}\\
  \includegraphics[width=\textwidth]{Images/FTAG/GN/HPO/fullmup2.png}
  \caption{Scan of the maximal learning rate ($x$-axis) versus initial learning rate (individual column) as measured by the validation loss ($y$-axis) of $SP$ models (top) and the $\mu$P model (bottom) with three different embedding widths: 64 (yellow), 128 (red), and 256 (purple). Taken from \cite{publicplotMUP}. The scan at LR initial = $10^{-5}$ benefitted from more test to capture the sudden rise in validation loss at larger LR max for SP.}
  \label{fig:fullSPmup}
\end{figure} 

The full LR init vs LR max scans can be found in Figure~\ref{fig:fullSPmup} for $SP$ and \gls{mup}. Changing the LR init has little effect on the reached performance, due to the LR scheduler quickly moving away from the initial value and the common LR end value of $10^{-5}$ shared by all models at the end of training. The LR max however is a significant hyperparameter having a large impact on performance. All $SP$ models with 256 embedding width are found to become unstable at large value of max LR. Note that the scan at LR initial = $10^{-5}$ benefitted from more test to capture the sudden rise in validation loss at larger LR max. As expected from the previous discussion, all \gls{mup} models stay stable, even at larger value of the learning rate. On the contrary, $SP$ models become unstable with large LR max. \gls{mup} models share the same optimal LR parameters, although some variance impacts the precision of the method on the smallest model. Due to limited computing power available, only one seed was run per-test, introducing some unmeasured statistical variance in the output. An essential conclusion in this respect is the computing gain from performing the \gls{hpo} on the smaller width model then the full-width one:
\begin{itemize}
  \item The full-width model (embedding size 256) has 2.3M parameters, taking $\sim$39 min per epoch on 2 A100 \gls{gpu}s each fed data by 20 \gls{cpu}s.
  \item The small-width model (embedding size 64) has 0.23M parameters, taking $\sim$20 min per epoch on 1 A100 \gls{gpu} fed data by 20 \gls{cpu}s.
\end{itemize}
Essentially, a single full-width model hyperparameter test is in computing terms equivalent to running 4 individual tests on the smaller model. Given a fixed computing budget, one can therefore have a far better coverage of the hyperparameter search space with $\mu$Transfer.\\

This optimisation study was carried out to demonstrate the benefits of \gls{mup} on \gls{gn2}. Interestingly, the optimal value found for both the \gls{mup} and $SP$ models is at an LR max = $5 \times 10^{-4}$ and LR initial = $10^{-5}$. The default values used in the prior training of \gls{gn2} were, by luck, the same LR max but a larger LR init of $10^{-7}$. To quantify the effect on performance, the $b$-efficiency versus $c$- and light-rejection on $t\bar{t}$ and $Z'$ of two \gls{mup} models are displayed in Figure~\ref{fig:rocmupGN2}, with the suboptimal one being the worst performing full-width model (LR max = $5 \times 10^{-5}$, LR init = $10^{-7}$) and the optimal one the best performing one (LR max = $5\times 10^{-5}$, LR init = $10^{-5}$). While the optimal and suboptimal models had close validation loss, respectively 0.601 and 0.591, a significant difference in background rejection at all efficiency is observed. At a $b$-tagging working point of 70\%, the suboptimal \gls{gn2} model underperforms the optimal one on $t\bar{t}$ by 18\% (14\%) on $c$-rejection (light-rejection) and the disparity is even higher on $Z'$, rising to 24\% (26\%) at a $b$-tagging working point of 30\% - which is equivalent to the 30\% \gls{wp} on $t\bar{t}$.

\begin{center}
  \begin{figure}[h!]
  \centerline{
  \includegraphics[width=0.50\textwidth]{Images/FTAG/GN/HPO/thesis_roc/roc_ttbar.png}
  \includegraphics[width=0.50\textwidth]{Images/FTAG/GN/HPO/thesis_roc/roc_zp.png}
  }
  \caption{The $c$- and light-rejections as a function of the $b$-jet tagging efficiency in the $t\bar{t}$ (left) and $Z'$ (right) test samples, from \cite{publicplotMUP}. Models compared are the optimal $\mu$P GN2 (LR max = $5\times 10^{-5}$, LR init = $10^{-5}$) and the suboptimal $\mu$P GN2 (LR max = $5 \times 10^{-5}$, LR init = $10^{-7}$), all with 256 embedding width. Shaded regions represent the binominal error band.}
  \label{fig:rocmupGN2}
  \end{figure}
\end{center}

Additional tests of \gls{mup} performed with \gls{gn2} showed a similar correct scaling across depth with similar optimal hyperparameters being transferrable, as expected from empirical results \cite{yang2021tuning}. Due to limited computing power available, the study of $SP$ versus \gls{mup} only encompassed two hyperparameters: the initial and maximal learning rate. The validity of the method has been confirmed and future studies optimising all the learning rate scheduler hyperparameter (including the warm-up and the learning rate at the end) will be carried out. Other hyperparameters that can best optimised with $\mu$Transfer are the initialisation variances of the different layers and the auxiliary objectives individual weights of Equation \ref{eq:totalobjgn}. To summarise this section on \gls{hpo}, the present work introduces two approaches that are combined to deliver an improved hyperparameter optimisation:
\begin{itemize}
  \item Executing the \gls{hpo} on KubeFlow with the Katib workload to benefit from state-of-the-art autoML algorithm.
  \item Leveraging the \gls{mup} parametrisation to increase the performance of the tuned \gls{gn2} and benefit from the factor 4 boost in hyperparameter test coverage from $\mu$Transfer.
\end{itemize}
The full optimisation of \gls{gn2} is, at the time of writing, an ongoing effort of the ATLAS Collaboration.

\subsection{GN2X: GN2 Variant for Boosted Higgs to Heavy Flavours Decays}\label{chap-GN2X}
A final aspect of the \gls{gn2} model presented in this thesis is an application of the architecture to a specialised objective: identifying boosted Higgs boson decaying into a pair of $b$- or $c$-quarks. Having an effective tagger to identify these boosted decays can significantly help analyses studying the decay of Higgs particles to a $c\bar{c}$ pair \cite{ATLAS:2022ers}, for the precise measurement of the Higgs boson $p_T$ spectrum \cite{PhysRevD.105.092003}, and for beyond the \gls{sm} measurements \cite{ATLAS:2023azi}. To perform this task, a new algorithm labelled GN2X is introduced based on the design of \gls{gn2} \cite{ATL-PHYS-PUB-2023-021}. Its main task is to discriminate jets from boosted Higgs boson decaying into a $b\bar{b}$ or a $c\bar{c}$ pair from those originating from the fully-hadronic top-quark decay and the multi-jet processes. While other taggers presented in this chapter relied on small-radius ($R=0.4$) PFlow jets or \gls{vr} jets, GN2X is trained on jet reconstructed with a large-radius ($R=1.0$) with \gls{ufo} objects to capture the majority of the decay products \cite{atlasLARGERJet}. \gls{ufo} combines PFlow \cite{atlasPFLOW} and Track-Calo clusters objects \cite{ATL-PHYS-PUB-2017-015}, thereby including neutral and charged components in the reconstruction. \gls{ufo} large-$R$ jets are reconstructed with the anti-$k_T$ algorithm with a radius $R = 1.0$ \cite{Cacciari:2008gp}. \\

To train the algorithm, Higgs produced in association with a $Z$-boson and decaying to a pair of heavy flavour quarks ($b\bar{b}$ or a $c\bar{c}$) are simulated. To not bias the result towards a specific $p_T$, $\eta$, and mass distributions of the jets, the simulation are resampled to have an approximately flat distribution of jet mass in the training set, while the validation set follow the \gls{sm} $ZH$ prodcution for a Higgs boson $H$ of a mass equal to 125 GeV. Similarly, the top-quark decay with subsequent hadronic decay of the $W$ boson in the $t \rightarrow bW$ chain is simulated for the training samples using a hypothetical $Z'$-boson of 4 TeV mass decaying as $Z' \rightarrow t\bar(t)$ with approximately flat jet $p_T$ distribution. The evaluation sample uses the \gls{sm} $t\bar{t}$ decay with filters on the scalar sum of the objects $p_T$ in the event. Finally, the multi-jet process is simulated in slices of particle-level jet $p_T$ to have the same spectrum. More details on the simulated samples used can be found in Ref. \cite{ATL-PHYS-PUB-2023-021}. After resampling the samples to enfroce the same $p_T$, $\eta$, and mass distributions, there are 62 million jets split between 15 million $H_{b\bar{b}}$, 15 million $H_{c\bar{c}}$, 10 million top, and 22 million multi-jets. \\

The previous algorithm for this task that now serves as benchmark in this study is the $X_{bb}$ tagger, a feed-forward network combining the flavour tagging discriminants of \gls{dl1r} or \gls{dl1d} for up to three \gls{vr} sub-jets associated to the large-$R$ jet \cite{ATL-PHYS-PUB-2020-019, ATL-PHYS-PUB-2021-035}. The track selection is similar to that of the GN-models (Section \ref{chap:GN}), and the inputs of the model are equivalent to those of Table~\ref{tab:gnInputVariables}, with the jet variables defined on the large-$R$ jet with the addition of the mass of the large-$R$ jet. At most 100 tracks associated with a jet are supplied to the network, as sorted by the decreasing transverse impact parameter significance $S_{d_0}$. The same auxiliary tasks as in \gls{gn2} are used with the same respective weights and neural network designs. The initialiser has an 192 embedding dimension and the transformer encoder combines 6 layers with 4 attention heads. The global representation is again obtained from a weighted sumer over the conditional tracks, with learnable attention weights. GN2X contains in total 1.5 million parameters, and is trained on 4 A100 \gls{gpu}s for 40 epochs ($\sim$1 hour per epoch) with a batchsize of 1000. 

\begin{center}
  \begin{figure}[h!]
  \centerline{
  \includegraphics[width=0.50\textwidth]{Images/FTAG/GN2X/roc/rocHbb.pdf}
  \includegraphics[width=0.50\textwidth]{Images/FTAG/GN2X/roc/rocHcc.pdf}
  }
  \caption{The ROC curves for $H(b\bar{b})$ (left) and $H(c\bar{c})$ tagging (right) on an SM simulated test samples, from \cite{ATL-PHYS-PUB-2023-021}. The respective tagging efficiency is displayed versus the top and multi-jet rejections, for jets with a $p_T > 250$ GeV and a mass $50 < m_J < 200$ GeV. Models compared are the baseline $X_{bb}$ tagger, using the variable-radius DL1r of at most 3 identified sub-jets in the large-$R$ jet, the tag obtained by combining the tag on two variable-radius jets within the large-$R$ jet with the single-jet GN2 tagger, and the GN2X model. The former is only available for $H(b\bar{b})$ tagging, and the $H(b\bar{b})$ rejection is displayed for $H(c\bar{c})$ tagging. The $H(c\bar{c})$ background is negligible for $H(b\bar{b})$ tagging. Shaded regions represent the binominal error band.}
  \label{fig:rocGN2X}
  \end{figure}
\end{center}
The model outputs four probabilities $p_{H_{b\bar{b}}}$, $p_{H_{c\bar{c}}}$, $p_{\textrm{top}}$, and $p_{\textrm{QCD}}$ that are combined in a discriminant score equivalent to Equations \ref{bdisc} and \ref{cdisc}: 
\begin{equation}
  D_{H_{b\bar{b}}} = \log \frac{p_{H_{b\bar{b}}}}{f_{H_{c\bar{c}}} . p_{H_{c\bar{c}}} + f_{\textrm{top}} . p_{\textrm{top}} + (1 - f_{H_{c\bar{c}}} - f_{\textrm{top}}) . p_{\textrm{QCD}}},
\end{equation}
where the flavour fractions were chosen from dedicated performance studies to be $f_{H_{c\bar{c}}} = 0.02$ and $f_{\textrm{top}} = 0.25$. A discriminant for $H_{c\bar{c}}$ is similarly defined:
\begin{equation}
  D_{H_{c\bar{c}}} = \log \frac{p_{H_{c\bar{c}}}}{f_{H_{b\bar{b}}} . p_{H_{b\bar{b}}} + f_{\textrm{top}} . p_{\textrm{top}} + (1 - f_{H_{b\bar{b}}} - f_{\textrm{top}}) . p_{\textrm{QCD}}},
\end{equation}
with $f_{H_{b\bar{b}}} = 0.3$ and $f_{\textrm{top}} = 0.25$. The performance of GN2X can be assessed from the \gls{roc} curves presented in Figure~\ref{fig:rocGN2X}. An additional performance to $X_{bb}$ and GN2X is presented, where two individual \gls{vr} sub-jets are $b$- or $c$-tagged by a \gls{vr}-trained \gls{gn2} model. The jets used are the leading \gls{vr} sub-jets associated to the large-$R$ jet. Note that $X_{bb}$ was not retrained on the specific samples but uses the \gls{vr}-trained \gls{dl1d} previously introduced. A clear performance gained is delivered by the GN2X method above both the $X_{bb}$ tagger and the combination of two individual tags with \gls{gn2}. The latter approach does not access correlations between the sub-jets, explaining its lower performance at higher $H(b\bar{b})$ and $H(c\bar{c})$ efficiencies than the GN2X and $X_{bb}$ model. At a 50\% $H(b\bar{b})$ \gls{wp}, GN2X improves the top rejection (multi-jet rejection) on $X_{bb}$ by a factor 1.6 (2.5) \cite{ATL-PHYS-PUB-2023-021}. For $H(b\bar{b})$ tagging, the $H(c\bar{c})$ background is negligible. GN2X also improves the performance for $H(c\bar{c})$ tagging over the approach combining two individual \gls{vr} tagged-jets: at a 50\% \gls{wp}, GN2X improves the top rejection by a factor 3, the multi-jet rejection by a fator 5, and the $H(b\bar{b})$ rejection by a factor 6. This novel approach to perform boosted object tagging is the first of its kind in ATLAS and is now integrated in the ATLAS software.

\section{Calibration}\label{chap-calibration}
All flavour taggers presented in this chapter are trained on \gls{mc}-simulated events, as described in Section \ref{ftagdatasets}. As such, they depend on and acquire specific features of the simulated data that might not be present in the real data collected by the ATLAS experiment. While the Collaboration aims to generate the highest-fidelity simulations possible thanks to advance software built on GEANT4 \cite{Agostinelli:602040} and many other specialised frameworks, inherent and unavoidable differences are left. To quantify the effect of using a simulation-trained network on real data, the ATLAS Collaboration performs Data-Monte Carlo agreement and calibration studies. These are performed in two steps: 
\begin{itemize}[leftmargin=*]
  \item Data-\gls{mc} \glspl{sf} are derived, comparing the output of the tagger on a simulated and real dataset using the same selection \cite{Aad:2019aic, ATLAS-CONF-2018-045, ATLAS-CONF-2018-006, cjettaggingCalib}. The efficiencies $\epsilon^f$ for each flavour $f \in {b, c, \textrm{light}}$ are measured, both on the simulated and real dataset, with \[\epsilon^f(p_T) = \frac{N^f_{\textrm{tagged}}(p_T)}{N^f_{\textrm{all}}(p_T)},\] where $N^f_{\textrm{tagged}}(p_T)$ is the number of jet of flavour $f$ in the bin of $p_T$ that are $b$-tagged and $N^f_{\textrm{all}}$ the total number of jet of flavour $f$ in the same bin. Scale factors to apply to simulations are then derived for each flavour $f$ as \[\textrm{SF}^f_{\textrm{Data-MC}}(p_T) = \frac{\epsilon^f_{\textrm{Data}}(p_T)}{\epsilon^f_{\textrm{MC}}(p_T)},\] giving the ratio of the measured efficiency in data over simulation. To include dynamics-dependent effects of the tagger, the efficiencies $\epsilon^f$ and \gls{sf} are derived in bins of jet $p_T$. Such calibration factors correct the efficiencies of tagging and mis-taging and are applied to all analyses using the flavour tagger. This calibration is performed independently for each output flavour of the tagger, as it relies on selecting a portion of the ATLAS data with a dominating proportion of the specific flavour. The $b$-tagging efficiency is derived from a sample of $t\bar{t}$ with two charged leptons in the final state, as described in Ref. \cite{Aad:2019aic}. The \gls{sf} for $c$-jet misstagging is calibrated on a $t\bar{t}$ sample decaying to exactly one charged lepton and several jets \cite{cjettaggingCalib}. Finally, the \gls{sf} for light-jets is derived in a sample of $Z$-bosons produced in association with jets ($Z+$jets) \cite{ATLAS:2023lwk}. Due to the extreme rejection power of modern flavour taggers, a special technique called \textit{flip tagger} is used for this last \gls{sf}, in which a tagger is modified to have a reduced light-rejection.
  \item \gls{mc}-\gls{mc} \glspl{sf} are derived between the chosen nominal Monte Carlo simulator used for training and other simulators or by changing the tuning \cite{ATL-PHYS-PUB-2020-009}. This dependency is measured by applying the same tagger to samples simulated with different generators, mainly \textsc{Pythia} \cite{SJOSTRAND2015159}, \textsc{Herwig} \cite{bellm2017herwig}, and \textsc{Sherpa} \cite{sherpa2.2paper} for variation to the parton shower and hadronisation and \textsc{MadGraph} for variation to the matrix element \cite{madgraph}. The decay chains of $b$- and $c$-hadrons in ATLAS is further simulated with the \textsc{EvtGen} package \cite{LANGE2001152}. These effects are measured into \glspl{sf} using the same technique as the data-\gls{mc} scale factors. For an alternative generator, the \glspl{sf} of flavour $f$ is derived by composing the Data-\gls{mc} \glspl{sf} with the nominal sample and the \gls{mc}-\gls{mc} \glspl{sf} as \[\textrm{SF}^f_{\textrm{Alternative}}(p_T) = \frac{\epsilon^f_{\textrm{Data}}(p_T)}{\epsilon^f_{\textrm{Nominal MC}}(p_T)} \times \frac{\epsilon^f_{\textrm{Nominal MC}}(p_T)}{\epsilon^f_{\textrm{Alternative MC}}(p_T)} = \frac{SF^f_{\textrm{data-MC}}(p_T)}{SF^f_{\textrm{MC-MC}}(p_T)}.\] 
\end{itemize}
These scale factors are applied in physics analyses as a per-jet weight to the discriminant. Some early studies of both scale factor types have been performed in Ref. \cite{ATL-PLOT-FTAG-2023-01}, showing good agreement between the data and simulated performance of \gls{dl1d} and \gls{gn1}. Variations due to the change of generator are also found to be at most of 8\% with respect to the nominal choice.

\section{Conclusion}
This chapter introduces the main machine learning models developped for heavy-flavour jets identification in ATLAS during the period covering 2020 to 2024. Work carried out in and presented in this thesis includes the first training of the \gls{dl1d} model, including the \gls{dips} sub-tagger of the first time in the ATLAS software. \gls{dl1d} is found to have improved background rejections at a fixed working point for both $b$- and $c$-tagging compared to the at-the-time released tagger: \gls{dl1r}. Significant changes in addition to the development of this new tagger were made to the preprocessing pipeline of the \textsc{Umami} framework \cite{UmamiCite} and the architecture as well as the list of features used. Finally, the new family of taggers based on graph neural network for \gls{gn1} and transformers for \gls{gn2} is presented, with the architecture adopted fully described and the performance of the different taggers compared. The hyperparameter optimisation of \gls{gn2} is also discussed, introducing the possibilities of using the new infrastructure of the KubeFlow server managed by \gls{cern} as well as the relevance of the maximal update parametrisation for improving the search for optimal hyperparameters of \gls{gn2}. Significant contributions were made to the development of the \textsc{Umami} and \textsc{Salt} frameworks used to train the DL1 and GN families of models were made to support these studies \cite{UmamiCite, SaltCite}. 

%\begin{tcolorbox}[colback=oxfordblue!5,colframe=blue!40!black,title=Summary of the Chapter]
%In this chapter, the \gls{ftag} world is introduced. 
%\end{tcolorbox}