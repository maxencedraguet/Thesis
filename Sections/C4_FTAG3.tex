
\subsection{Optimising GN2}\label{chap-GN2Opt}
The state-of-the-art flavour tagger at ATLAS is, at the time of writing, built on the \gls{gn2} architecture. Naturally, fine-tuning the model is required to further push the performance higher. Many studies are currently being carried out to deliver yet a stronger tagger than the \gls{gn2} version presented in this thesis. A non-exhaustive lists of ongoing research directions include: 
\begin{itemize}
  \item Optimising the track selection and the jet reconstruction type. Moving towards yet a looser selection and letting the network sift through a larger set of background tracks could deliver further performance. Assessing the effect of modelling uncertainties is however of particular importance for these modiciations.
  \item The inclusion of neutral constituent information. Tracks are reconstructed from hits in semiconductor-based detectors. Such hits are only recorded for charged particles flying through the active regions of the sensors. This entirely misses neutral particles, from neutrons, neutral pions and kaons, and neutrinos. All but the latters leave energy in the calorimeters that is measurable and accessible. Studies are ongoing to add this information to the set of tracks. 
  \item The inclusion of leptonic information. 40\% of $b$-hadrons include either an $e$ or a $\mu$ in the jet cone \cite{Tanabashi:2018oca}. As seen with \gls{gn1}, the inclusion of a leptonic information in the set of tracks leads to a significant performance increase. Studies are ongoing to build a finer lepton-information analyser within \gls{gn2}.
  \item Hadronic decays of $\tau$ are a major source of backgrounf for analysis focusing on $c$-jet tagging, due to similar signatures. Including theses leptonic decays in the classification objective has been seen to deliver promising results in initial studies. 
  \item Finer output classes categorisation. Currently, the simple labelling scheme requires combining topologies with significant differences. For example, purely hadronic and semi-leptonic decays of $b$-jets are both labelled $b$-jets. Adopting a greater flexibility in the definition of classes allows the model to fully utilise the unique signature of each process. 
  \item Integrating further expert information into the design is known to deliver a great boost to performance. Studies are ongoing to upgrade the set of auxiliary tasks, in particular for secondary vertex fitting and reconstruction. A \gls{gn2} model able to reliably reconstruct this information would have a use case in the ATLAS experiment beyond heavy-flavour jet tagging, while benefitting from improved performance for this essential task.  
\end{itemize}

Larger design considerations as studied in the above list are paramount to a well-functioning tagger. An equally essential endeavour is to fine-tunie an architecture to extract the best performance from a chosen data processing strategy. This section focuses on some initial studies to perform \gls{hpo} and network architecture search for \gls{gn2}. The essential challenge is that a test of higher a change to a hyperparameter or to the model architecture requires to fully train a gls{gn2} model with the new choices. This is a costly process, as a single epoch of the \gls{gn2} takes roughly $\sim$28 min for 2 NVIDIA A100 \gls{gpu} each fed data by 20 \gls{cpu} to perform 1 epoch on a 30M jets dataset with batchsize 2k split on the \gls{gpu}. \gls{gn2} has many hyperparameters that should be optimised to deliver optimal performance, among which the most relevant are: initial $lr$, maximal $lr$, end $lr$, the weights of the 2 auxiliary tasks, the amount of weight decay, the batchisze, and the float precision. Architecture-level changes are the embedding dimension (output of the initialiser and as input and output of each transformer encoder), the depth of the initialiser, the number of layers and heads in the transformer encoder, the size of the transformer output, the auxiliary tasks \gls{dnn}, the activation functions, and the specific loss functions and their class-weights used. \\

Unfortunately, access to \gls{gpu} was, at the time of writing, limited for members of the Collaboration. Most of the computing power leveraged to train advanced \gls{ml} models such as \gls{gn2} is accessed on high-performance cluster of institutes members belong to. In this respect, a promising area of development is being pursued by \gls{cern}, with the introduction of a Kubeflow-backed served hosted on \textit{ml.cern.ch} \cite{KubeflowCern}. Kubeflow, created by Google and now backed by the Cloud Native Computing Foundation, is an open-source framework built on Kubernetes to perform machine learning operations such as training, inference, deployment, and hyperparameter optimisation. The project aims to centralise the \gls{gpu} resources of the different \gls{cern} experiments into a single centralised cluster with datastorage, efficient I/O reading capabilities, and dedicated \gls{gpu} nodes. Katib, Kubeflow's dedicated \gls{hpo} workload, is a promising approach to perform effective hyperparameter optimisation with state-of-the-art autoML techniques, which automate and refine the strategy to test and converge on the best hyperparameters \cite{george2020katib}. At the time of writing, the server was still in a testing phase with little hardware accessible, thereby removing it from consideration as a possible solution to carry out the full \gls{hpo} of \gls{gn2}. However, the salt framework \gls{gn2} is trained with was adapted to run on Kubeflow platform and tests showed promising possibilites for the Collaboration. Being accessible to any member of the ATLAS Collaboration, it greatly ``democratises'' access to projects requiring powerful computing power for insitutes lacking a \gls{hpc}. \\

Large \gls{nn}, for example large language models, develop in the future at ATLAS will require cluster designed for machine learning, with many \gls{gpu} accessible on dedicated nodes for splitting the process. This paradigm of computing is markedly different from the typical grid-base distributed computing deployed in particle physics experiments. While \gls{mc}-based samples and sub-sampled datasets can be effectivly processed in parallel by autonomous parallel jobs, Machine learning however requires communication between the different jobs to keep the weights of the model being updated during training synchronised on the different \gls{gpu}. A fast connexion between these \gls{gpu} is essential, as is having fast read access to the full datasets due to the need to loop over the whole data for each epoch during training. Distributing the computation across different \gls{hpc} geographically distant, as is common with the current \gls{cern} computing grid, is not effective for this purpose. The \gls{cern} Kubeflow server is an exciting area of development for future computational needs of ATLAS. Furthermore, having a framework compatible with Kubeflow allows operating on multiple platforms, giving the flexibility to scale resource access for computationally demanding tasks, such as \gls{hpo}. For example, Google Cloud is Kubeflow-compatible and host a larger amount of \gls{gpu}, but so are other large cloud providers such as Amazon Web Service and Microsoft's Azure. Salt can be effectivly trained on one of these cloud prodividers or the \gls{cern}'s Kubeflow with no noticeable distinction for the user. \\

While leveraging a large amount of computing power is the natural solution to the challenging task of \gls{hpo} of a ``large'', by ATLAS standards, neural network models, a more refined technique can be exploited in the present case. Recent works suggest that the optimal hyperparameters of a nominal model can be estimated from a smaller model \cite{yang2021tuning}. Here smaller refers to either the depth - the number of layers - or the width - the number of neurons per layer or, in the case of a transformer, the number of heads in the multihead attention - of the neural network. Ref \cite{pmlr-v139-yang21c} establishes the mathematical foundation backing this surprising behaviour of deep neural network: the \gls{mup}. The rest of this section is dedicated to introducing and defining the maximal update parametrisation before establishing its relevance for \gls{hpo}. \\

\subsubsection{Maximal Update Parametrisation}
The maximal update parametrisation is first and foremost a \textit{parametrisation}. In this context, the parametrisation of a neural network refers to the definition of the weights of each individual neurons, the way they are initialised, and how they are updated from a given optimisation algorithms, such as Adam or \gls{sgd} \cite{adamPaper}. In the presented context, the default or \textit{standard} parametrisation (SP) refers to a parametrisation of the weights following the so-called LeCun parametrisation \cite{LeCun2012}. Such a parametrisation, routinely deployed in \gls{ml} frameworks such as PyTorch \cite{pytorch}, initialises the weights by sampling them from a Gaussian with mean 0 and standard deviation being the inverse of the input dimension of the layer the weight belongs to. For both Adam and \gls{sgd}, a single master learning rate (LR) $\eta$ is used when updating all weights. For \gls{mup}, some subtle difference are applied, as summarised in Table~\ref{tab:mupvsspdef}. Mainly, the output layer weights are sampled from a Gaussian with a standard deviation being the inverse of the input dimension \textbf{squared} of the output layer. Concerning the learning rates, the hidden and output layers are scaled down by their respective input dimension for Adam. For \gls{sgd}, the output layer LR is scaled similarly, but the input and the bias LR are scaled up by the output dimension of the layers. 

\begin{table}[h]
  \begin{center}
      \begin{tabular}{c|cc|cc|cc} 
      	 \hline \hline
          & \multicolumn{2}{c|}{Initialisation Distribution} & \multicolumn{2}{c|}{Adam LR} & \multicolumn{2}{c}{SGD LR}  \\
            & SP  & \gls{mup} & SP  & \gls{mup} & SP  & \gls{mup}  \\ \hline
          $w^{L_\textrm{inp}}$ & $\sim$ $\mathcal{N}\left(0, \frac{1}{d^{\textrm{in}}_{L_\textrm{inp}}}\right)$ & $\sim$ $\mathcal{N}\left(0, \frac{1}{d^{\textrm{in}}_{L_\textrm{inp}}}\right)$                                         & $\eta$ & $\eta$                                    & $\eta$ & $\eta \times d^{\textrm{out}}_{L_\textrm{inp}}$ \\ 
          $w^{L_\textrm{hid}}$ & $\sim$ $\mathcal{N}\left(0, \frac{1}{d^{\textrm{in}}_{L_\textrm{hid}}}\right)$ & $\sim$ $\mathcal{N}\left(0, \frac{1}{d^{\textrm{in}}_{L_\textrm{hid}}}\right)$                                         & $\eta$ & $\eta / d^{\textrm{in}}_{L_\textrm{hid}}$ & $\eta$ & $\eta$                                    \\ 
          $w^{L_\textrm{out}}$ & $\sim$ $\mathcal{N}\left(0, \frac{1}{d^{\textrm{in}}_{L_\textrm{out}}}\right)$ & $\sim$ $\mathcal{N}\left(0, \frac{1}{d^{\textrm{in}}_{L_\textrm{out}}\times d^{\textrm{in}}_{L_\textrm{out}}}\right)$  & $\eta$ & $\eta / d^{\textrm{in}}_{L_\textrm{out}}$ & $\eta$ & $\eta / d^{\textrm{in}}_{L_\textrm{out}}$ \\
          $b^{L} \;\forall L$ & 0 & 0                                                                                                                                                                                          & $\eta$ & $\eta$                                    & $\eta$ & $\eta \times d^{\textrm{out}}_{L}$              \\  \hline \hline
      \end{tabular}
    \caption{Comparing the Standard Parametrisation (SP) to the Maximal Update Parametrisation ($\mu P$), as defined in Ref \cite{yang2021tuning} based on the work of Ref \cite{pmlr-v139-yang21c}.}
    \label{tab:mupvsspdef}
  \end{center}
\end{table}

This particular derivation of \gls{mup}, derived in Ref \cite{yang2021tuning}, is equivalent to the original \gls{mup} derivation spelled out in Ref \cite{pmlr-v139-yang21c}. \gls{mup} turns out to be the unique parametrisation that maximally updates the weights of a neural network, where ``\textit{maximal update}'' refers to the size of the update of a network of width tending towards infinity. For such a hypotethical model useful only for theorical considerations, the updates naturally must be independent of the width, otherwise they would become infinite leading the model to be unstable. For the specific case of the attention mechanism as present in the multihead attention of transformers, the scaling has to be modified from $\sqrt{d_k} \rightarrow d_k$ to properly scale with width, as shown in Ref \cite{yang2021tuning}. Figure~\ref{fig:muspweights} shows a comparison of a \gls{gn2} model with \gls{mup} parametrisation to a standardly parametrised \gls{gn2}, referred to as the SP model. Each curve displays, for different embedding width in the transformer and the track initialiser, the sum of the absolute values of the weights before the activation ($L_1(\textrm{layer}) = \sum_{w_i \in \textrm{layer}} |w_i|$) for the initialiser and transformer models only. Three timesteps are displayed for each model, the initialisation ($t=1$) and after 1 ($t=2$) and 2 ($t=2$) training steps. The interesting behaviour highlighted in this figure is that for the SP model, the pre-activation weights blow up with width during training. For \gls{mup} however, the $L_1$ of each layer stays flat with width even during the training, proving the correct parametrisation of the model and the ``width-independent'' scaling. This unstable behaviour of the SP parametrisation is easily highlighted thanks to the use of a large and fixed learning rate (here $lr = 10^{-2}$). 

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{Images/FTAG/GN/HPO/spweights.png}\\
  \includegraphics[width=\textwidth]{Images/FTAG/GN/HPO/mupweights.png}
  \caption{The sum of the absolute value of the pre-activation weights for the different layers in the initialiser and transformer parts of a GN2-like model in standard parametrisation (SP - top) and in $\mu$P parametrisation (bottom), at three timesteps: initialisation ($t=1$ - left), after one training step with $lr = 10^{-2}$ ($t=2$ - centre), and a second training step ($t=3$). Taken from \cite{publicplotMUP}. The models displayed are labelled GN2-like as they lack auxiliary tasks. }
  \label{fig:muspweights}
\end{figure} 

Theoretically, a \gls{mup} model should deliver equal to better performance to an equivalent SP model when both have optimal hyperparameters. This behaviour is due to the maximal updating of the former, leading to optimal in-depth updates of all layers. The standard parametrisation does not implement this correct updating, with outter layers closer to the loss function having an opaque effect on the propagation of the update for the input layers proportionally to their widths. Scaling down the learning rate is not a sufficient modification to correc SP: as displayed in Figure~\ref{fig:muspweights}, not all layers update incorrectly with some pre-activation sum staying flat across width. By updating all activation maximally indepentely of the width, \gls{mup} should outperform SP for a tuned learning rate \cite{pmlr-v139-yang21c}. A significant advantage of this width independent effect is that the optimal learning rate for a \gls{mup} architecture becomes width-independent. This leads to the $\mu$Transfer algorithm for \gls{hpo}: one can search for the best hyperparameters on a \gls{mup} model with fewer neurons per layers (smaller width) and transfer the found optimal to the full-size model at no extra cost (0-transfer) \cite{yang2021tuning}. The benefit of adopting the maximal update parametrisation are:
\begin{enumerate}
  \item Better performance of a \gls{mup} model compared to an SP model for a tuned learning rate.
  \item Improved hyperparameter optimisation with the $\mu$Transfer algorithm: performing the \gls{hpo} scan on a smaller and easier to train model to 0-transfer the best set of hyperparameters to the full-size models. 
  \item Better hardware usage for \gls{hpo}: a smaller model can be trained on a single \gls{gpu}. This is of particular interest for the ATLAS Collaboration, as most of the \gls{gpu} resource accessible is scattered on geographically distant computing tiers and not on single nodes.  
  \item Simplified architecture: in the \gls{mup}, a wider model outperforms a smaller model if no overtraining occurs. Therefore, the best learning rate hyperparameter has to be found ounce for all \gls{gn2} model of varying widths.  
\end{enumerate}

Hyperparameters that can be optimised with the $\mu$Transfer algorithms are said to be $\mu$Transferable. They consist of \cite{yang2021tuning}: 
\begin{itemize}
  \item Learning rate and parameters of a learning rate scheduler.
  \item Optimiser parameters (momentum, Adam $\alpha$ and $\beta$).
  \item Initialisation parameters (initial per-layer variance).
  \item Multiplicative constants.
\end{itemize}
Many parameters unfortunately do not $\mu$Tranfer as they combine aspects of the model and the data. They must be studied on the full size model directly. For example, the regularisation parameters (dropout, weight decay, normalisation, ...) do not scale, as a particular model size will overfit depending on the data. Finally, the last important family of hyperparameters are those defining the scale of the problem. These parameters are not found from $\mu$Transfer but rather ``$\mu$Transfered along''. They consist of the width (number of neurons per layer, number of attention heads in a transformer, ...), the depth, and the batchsize. Only the scaling along width is theoretically justified thanks to \gls{mup}, while the others are empiraclly observed to hold \cite{yang2021tuning}.\\

Studies of the \gls{mup} parametrisation and $\mu$Transfer algorithm were performed for the \gls{gn2} flavour tagger. In this architecture, the most relevant dimensions are the width and the depth of the transformer part, tasks with building a global conditional representation of the tracks from the embedded tracks processed by the initialiser network. These two dimensions are keys as most of the parameters of the \gls{gn2} model are in the transformer and the initialiser, with only few parameters set in the networks of the primary and auxiliary tasks. As such, the dimension scaled with $\mu$Transfer is the embedding width. The number of parameters in the tranformer roughly scales with the square of the embedding width due to the attention mechanism, making it the most sensitive parameter to define the complexity of \gls{gn2}. 

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{Images/FTAG/GN/HPO/maincompmupsp.png}
  \caption{Comparison of a maximal learning rate value scan at an initial learning rate value of $10^{-5}$ for an SP (left) and a \gls{mup} GN2 models (right) for three different embedding widths: 64 (yellow), 128 (red), and 256 (purple). The $y$-axis displays the validation loss attained. Taken from \cite{publicplotMUP}.}
  \label{fig:maincompmupsp}
\end{figure} 

To demonstrate the effect of \gls{mup} on \gls{gn2}, a learning rate hyperparameter optimisation campaign targeting the initial and maximal value of the learning rate (the final value - LR end - was not modified and is kept at $10^{-5}$ for all test due to limited compute) is performed using the standard and maximal update parameterisation (SP vs \gls{mup}). Three embedding widths are considered: the nominal 256 embedding width, defining a \gls{gn2} model with 2.3M parameters, a mid-size 128 embedding width (0.72M parameters), and a small 64 embedding width model with 0.23M parameters. Interestingly, this smaller model with an embedding 1/4 of the full model only has a 10th of the parameters. Furthermore, the small model was found to be trainable on a single \gls{gpu} while the full and mid-size models required two \gls{gpu} to be trained in a reasonable amount of time. All models are full \gls{gn2} models (with auxiliary tasks) trained on 30M PFlow jets composed 60\% $t\bar{t}$ and 40\% $Z'$ for 40 epochs with batchsize 1024. All parameters not mentioned are kept similar between embedding widths and parametrisation, and the epoch giving the lowest validation loss is picked for each model. Figure~\ref{fig:maincompmupsp} displays the main result from this campaign, displaying the various LR max considered at the best LR initial found ($10^{-5}$). Three main observations can be drawn from analysing the result:
\begin{enumerate}
  \item With \gls{mup}, the wider \gls{gn2} model - larger embedding width - outperforms the smaller version. 
  \item Wider models do not always outperform smaller model with SP. In particular at large LR max the wider model becomes unstable and its performnce in terms of validation loss significantly decreases.
  \item The optimal LR max (and LR init as shown in Figure~\ref{fig:fullSPmup}) are shared across width with \gls{mup}, while no such behaviour is guaranteed for SP - but is observed in the present case.
\end{enumerate}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{Images/FTAG/GN/HPO/fullSP2.png}\\
  \includegraphics[width=\textwidth]{Images/FTAG/GN/HPO/fullmup2.png}
  \caption{Scan of the maximal learning rate ($x$-axis) versus initial learning rate (individual column) as measured by the validation loss ($y$-axis) of SP models (top) and the $\mu$P model (bottom) with three different embedding widths: 64 (yellow), 128 (red), and 256 (purple). Taken from \cite{publicplotMUP}. The scan at LR initial = $10^{-5}$ benefitted from more test to capture the sudden rise in validation loss at larger LR max for SP.}
  \label{fig:fullSPmup}
\end{figure} 

The full LR init vs LR max scans can be found in Figure~\ref{fig:fullSPmup} for SP and \gls{mup}. Changing the LR init has little effect on the reached performance, due to the LR scheduler quickly moving away from the initial value and the common LR end value of $10^{-5}$ shared by all model at the end of training. The LR max however is a significant hyperparameter having a large impact on performance. All SP models with 256 embedding width are found to become unstable at large value of max LR. Note that the scan at LR initial = $10^{-5}$ benefitted from more test to capture the sudden rise in validation loss at larger LR max. As expected from the previous discussion, all \gls{mup} models stay stable, even at larger value of the learning rate. On the contrary, SP models become unstable with large LR max. \gls{mup} models share the same optimal LR parameters, although some variance impacts the precision of the method on the smallest model. Due to limited computing power available, only one seed was run per-test, introducing some unmeasurable statistical variance in the output. An essential conclusion is this respect is the computing gain from performing the \gls{hpo} on the smaller width model then the full width one:
\begin{itemize}
  \item The full width model (embedding size 256) has 2.3M parameters, taking $\sim$39 min per epoch on 2 A100 \gls{gpu}s each fed data by 20 \gls{cpu}s.
  \item The small width model (embedding size 64) has 0.23M parameters, taking $\sim$20 min per epoch on 1 A100 \gls{gpu} fed data by 20 \gls{cpu}s.
\end{itemize}
Essentially, a single full width model hyperparameter test is in computing terms equivalent to running 4 individual tests on the smaller model. Given a fixed computing budget, one can therefore have a far better coverage of the hyperparameter search space with $\mu$Transfer.

This optimisation study was carried out to demonstrate the benefits of \gls{mup} on \gls{gn2}. Interestingly, the optimal value found for both the \gls{mup} and SP models is at an LR max = $5 \times 10^{-4}$ and LR initial = $10^{-5}$. The default values used in the prior training of \gls{gn2} were, by luck, the same LR max but a larger LR init of $10^{-7}$. To quantify the effect on performance, the $b$-efficiency versus $c$- and light-rejection on $t\bar{t}$ and $Z'$ of two \gls{mup} models are displayed in Figure~\ref{fig:rocmupGN2}, with the suboptimal one being the worst performing full-width model (LR max = $5 \times 10^{-5}$, LR init = $10^{-7}$) and the optimal one the best performing one (LR max = $5\times 10^{-5}$, LR init = $10^{-5}$). While the optimal and suboptimal models had close validation loss, respectively 0.601 and 0.591, a significant difference in background rejection at all efficiency is observed. At a $b$-tagging working point of 70\%, the suboptimal \gls{gn2} model underperforms the optimal one on $t\bar{t}$ by 18\% (14\%) on $c$-rejection (light-rejection) and the disparity is even higher on $Z'$, rising to 24\% (26\%) at a $b$-tagging working point of 30\% - which is equivalent to the 30\% \gls{wp} on $t\bar{t}$.

\begin{center}
  \begin{figure}[h!]
  \centerline{
  \includegraphics[width=0.50\textwidth]{Images/FTAG/GN/HPO/thesis_roc/roc_ttbar.png}
  \includegraphics[width=0.50\textwidth]{Images/FTAG/GN/HPO/thesis_roc/roc_zp.png}
  }
  \caption{The $c$- and light-rejections as a function of the $b$-jet tagging efficiency in the $t\bar{t}$ (left) and $Z'$ (right) test samples, from \cite{publicplotMUP}. Models compared are the optimal $\mu$P GN2 (LR max = $5\times 10^{-5}$, LR init = $10^{-5}$) and the suboptimal $\mu$P GN2 (LR max = $5 \times 10^{-5}$, LR init = $10^{-7}$), all with 256 embedding width. Shaded regions represent the binominal error band.}
  \label{fig:rocmupGN2}
  \end{figure}
\end{center}

More preliminary tests of \gls{mup} were performed with \gls{gn2}, showing good scaling across depth as expected from empirical results \cite{yang2021tuning}. Due to limited computing power available, the study of SP versus \gls{mup} only encompassed two hyperparameters: the initial and maximal learning rate. The validity of the method has been confirmed and future studies optimising all of the learning rate scheduler hyperparameter (including the warm-up and the learning rate at the end) will be carried out. Other hyperparameters that can best optimised with $\mu$Transfer are the initialisation variances of the different layers and the auxiliary objectives individual weights of Equation \ref{eq:totalobjgn}. In summary, the present work introduces two approaches that are combined to deliver an improved hyperparameter optimisation:
\begin{itemize}
  \item Carrying out the \gls{hpo} on Kubeflow with the Katib workload to benefit from state-of-the-art autoML algorithm.
  \item Leveraging the \gls{mup} parametrisation to increase the performance of the tuned \gls{gn2} and benefit from the factor 4 boost in hyperparameter test coverage from $\mu$Transfer.
\end{itemize}
The full optimisation of \gls{gn2} is, at the time of writing, an ongoing effort of the ATLAS Collaboration.

\subsection{GN2X: a GN2 variant for Boosted Higgs Bosons Decay to Heavy Flavours}\label{chap-GN2X}
A final aspect of the \gls{gn2} model presented in this thesis is an application of the architecture to a specialised objective: identifying boosted Higgs boson decaying into a pair of $b$- or $c$-quarks. Having an effective tagger to identify these boosted decays can significantly help analyses studying the decay of Higgs particles to a $c\bar{c}$ pair \cite{ATLAS:2022ers}, for the precise measurement of the Higgs boson $p_T$ spectrum \cite{PhysRevD.105.092003}, and for beyond the \gls{sm} measurements \cite{ATLAS:2023azi}. To perform this task, a new algorithm labelled GN2X is introduced based on the design of \gls{gn2} \cite{ATL-PHYS-PUB-2023-021}. Its main task is to discriminate jets from boosted Higgs boson decaying into a $b\bar{b}$ or a $c\bar{c}$ pair from those originating from the fully-hadronic top-quark decay and the multijet processes. While other taggers presented in this chapter relied on small-radius ($R=0.4$) PFlow jets or \gls{vr} jets, GN2X is trained on jet reconstructed with a large-radius ($R=1.0$) with \gls{ufo} objects to capture the majority of the decay products \cite{atlasLARGERJet}. \gls{ufo} combines PFlow \cite{atlasPFLOW} and Track-Calo clusters objects \cite{ATL-PHYS-PUB-2017-015}, thereby including neutral and charged components in the reconstruction. \gls{ufo} large-$R$ jets are reconstructed with the anti-$k_T$ algorithm with a radius $R = 1.0$ \cite{Cacciari:2008gp}. \\

To train the algorithm, Higgs produced in association with a $Z$ boson and decaying to a pair of heavy flavour quarks ($b\bar{b}$ or a $c\bar{c}$) are simulated. To not bias the result towards a specific $p_T$, $\eta$, and mass distributions of the jets, the simulation are biased through sampling to have an approximately flat distribution of jet mass in the training set, while the validation set follow the \gls{sm} $ZH$ prodcution for a Higgs boson $H$ of a mass equal to 125 GeV. Similarly, the top-quark decay with subsequent hadronic decay of the $W$ boson in the $t \rightarrow bW$ chain is simulated for the training sample using a hypothetical $Z'$ boson of 4 TeV mass decaying as $Z' \rightarrow t\bar(t)$ with approximately flat jet $p_T$ distribution. The evaluation sample uses the \gls{sm} $t\bar{t}$ decay with filters on the scalar sum of the objects $p_T$ in the event. Finally, the multijet process is simulated in slices of particle-level jet $p_T$ to have the same spectrum. More details on the simulated samples used can be found in Ref \cite{ATL-PHYS-PUB-2023-021}. After resampling the samples to ahve similar $p_T$, $\eta$, and mass distributions, there are 62 million jets split between 15 million $H_{b\bar{b}}$, 15 million $H_{c\bar{c}}$, 10 million top, and 22 million multijets. \\

The previous algorithm for this task that now serves as benchmark in this study is the $X_{bb}$ tagger, a feed-forward network combining the flavour tagging discriminants of \gls{dl1r} or \gls{dl1d} for up to three \gls{vr} subjets associated to the large-$R$ jet \cite{ATL-PHYS-PUB-2020-019, ATL-PHYS-PUB-2021-035}. The track selection is similar to that of the GN-models (Section \ref{chap:GN}), and the inputs of the model are equivalent to those of Table~\ref{tab:gnInputVariables}, with the jet variables defined on the large-$R$ jet and an additional jet variable being used: the mass of the large-$R$ jet. At most 100 tracks associated with a jet are supplied to the network, as sorted by the decreasing transverse impact parameter significance $S_{d_0}$. The same auxiliary tasks as in \gls{gn2} are used with the same respective weights and neural network designs. The initialiser has an 192 embedding dimension and the transformer encoder combines 6 layers with 4 attention heads. The global representation is again obtained from a weighted sumer over the conditional tracks, with learnable attention weights. GN2X contains in total 1.5 million parameters, and is trained on 4 A100 \gls{gpu}s for 40 epochs ($\sim$1 hour per epoch) with a batchsize of 1000. 

\begin{center}
  \begin{figure}[h!]
  \centerline{
  \includegraphics[width=0.50\textwidth]{Images/FTAG/GN2X/roc/rocHbb.pdf}
  \includegraphics[width=0.50\textwidth]{Images/FTAG/GN2X/roc/rocHcc.pdf}
  }
  \caption{The ROC curves for $H(b\bar{b})$ (left) and $H(c\bar{c})$ tagging (right) on an SM simulated test samples, from \cite{ATL-PHYS-PUB-2023-021}. The respective tagging efficiency is displayed versus the top and multijet rejections, for jets with a $p_T > 250$ GeV and a mass $50 < m_J < 200$ GeV. Models compared are the baseline $X_{bb}$ tagger, using the variable-radius DL1r of at most 3 identified subjet in the large-$R$ jet, the tag obtained by combining the tag on two variable-radius jets within the large-$R$ jet with the single-jet GN2 tagger, and the GN2X model. The former is only available for $H(b\bar{b})$ tagging, and the $H(b\bar{b})$ rejection is displayed for $H(c\bar{c})$ tagging. The $H(c\bar{c})$ background is negligible for $H(b\bar{b})$ tagging. Shaded regions represent the binominal error band.}
  \label{fig:rocGN2X}
  \end{figure}
\end{center}

To discriminate, the model outputs four probabilities $p_{H_{b\bar{b}}}$, $p_{H_{c\bar{c}}}$, $p_{\textrm{top}}$, and $p_{\textrm{QCD}}$ that are combined in a discriminant score equivalent to Equations \ref{bdisc} and \ref{cdisc}: 
\begin{equation}
  D_{H_{b\bar{b}}} = \log \frac{p_{H_{b\bar{b}}}}{f_{H_{c\bar{c}}} . p_{H_{c\bar{c}}} + f_{\textrm{top}} . p_{\textrm{top}} + (1 - f_{H_{c\bar{c}}} - f_{\textrm{top}}) . p_{\textrm{QCD}}},
\end{equation}
where the flavour fractions were chosen from dedicated performance studies to be $f_{H_{c\bar{c}}} = 0.02$ and $f_{\textrm{top}} = 0.25$. A discriminant for $H_{b\bar{b}}$ is similarly defined:
\begin{equation}
  D_{H_{c\bar{c}}} = \log \frac{p_{H_{c\bar{c}}}}{f_{H_{b\bar{b}}} . p_{H_{b\bar{b}}} + f_{\textrm{top}} . p_{\textrm{top}} + (1 - f_{H_{b\bar{b}}} - f_{\textrm{top}}) . p_{\textrm{QCD}}},
\end{equation}
with $f_{H_{b\bar{b}}} = 0.3$ and $f_{\textrm{top}} = 0.25$. The performance of GN2X can be assessed from the \gls{roc} curves presented in Figure~\ref{fig:rocGN2X}. An additional performance to $X_{bb}$ and GN2X is presented, where two individual \gls{vr} subjets are $b$- or $c$-tagged by a \gls{vr}-trained \gls{gn2} model. The jets used are the leading \gls{vr} subjets associated to the large-$R$ jet. Note that $X_{bb}$ was not retrained on the specific samples.


A clear performance gained is delivered by the GN2X method above both the $X_{bb}$ tagger and the combination of two individual tags with \gls{gn2}. The latter approach does not access correlations between the subjets, explaining its lower performance at higher $H(b\bar{b})$ and $H(c\bar{c})$ efficiencies than the GN2X and $X_{bb}$ model. At a 50\% $H(b\bar{b})$ \gls{wp}, GN2X improves the top rejection (multijet rejection) on $X_{bb}$ by a factor 1.6 (2.5) \cite{ATL-PHYS-PUB-2023-021}. For $H(b\bar{b})$ tagging, the $H(c\bar{c})$ background is negligible. GN2X also improves the performance for $H(c\bar{c})$ tagging over the approach combining two individual \gls{vr} tagged-jets: at a 50\% \gls{wp}, GN2X improves the top rejection by a factor 3, the multijet rejection by a fator 5, and the $H(b\bar{b})$ rejection by a factor 6. This novel approach to perform boosted object tagging is the first of its kind in ATLAS and is now integrated in the ATLAS software.

\section{Calibration}\label{chap-calibration}
All flavour taggers presented in this chapter have been trained on \gls{mc} simulations, as described in Section \ref{ftagdatasets}. As such, they depend on and acquire specific features of the simulated data that might not be present in the real data collected by the ATLAS experiment. While the Collaboration aims to generate the highest-fidelity simulation possible with advance software build on GEANT4 \cite{Agostinelli:602040} and many other specialised framework, inherent and unavoidable differences are left. To quantify the effect of using a simulation-trained network on real data, the ATLAS Collaboration performs Data-Monte Carlo agreement and calibration studies. These are performed in two steps: 
\begin{itemize}
  \item Data-\gls{mc} Scale Factor (\gls{sf}) are derived, comparing the output of the tagger on a simulated and real dataset with equivalent selection \cite{Aad:2019aic, ATLAS-CONF-2018-045, ATLAS-CONF-2018-006, cjettaggingCalib}. The efficiencies $\epsilon^f$ for each flavour $f \in {b, c, \textrm{light}}$ are measured, both on the simulated and real dataset, with \[\epsilon^f(p_T) = \frac{N^f_{\textrm{tagged}}(p_T)}{N^f_{\textrm{all}}(p_T)},\] where $N^f_{\textrm{tagged}}(p_T)$ is the number of jet of flavour $f$ in the bin of $p_T$ that are $b$-tagged and $N^f_{\textrm{all}}$ the total number of jet of flavour $f$ in the same bin. A scale factor is then derived for each flavour $f$ as \[\textrm{SF}^f_{\textrm{Data-MC}}(p_T) = \frac{\epsilon^f_{\textrm{Data}}(p_T)}{\epsilon^f_{\textrm{MC}}(p_T)},\] giving the ratio of the measured efficiency in data over simulation. To include dynamic effect of the taggers, the efficiencies $\epsilon^f$ and \gls{sf} are derived in bins of jet $p_T$. Such calibration factors correct the efficiencies of tagging and misstaging and are applied to all analyses using the flavour tagger. This calibration is performed independently for each output flavour of the tagger, as it relies on selecting a portion of the ATLAS data with a large proportion of the studied flavour. The $b$-tagging efficiency is derived from a sample of $t\bar{t}$ with two charged leptons in the final state, as described in Ref \cite{Aad:2019aic}. The \gls{sf} for $c$-jet misstagging is calibrated on a $t\bar{t}$ sample decaying to exactly one charged lepton and several jets \cite{cjettaggingCalib}. Finally, the \gls{sf} for light-jets is derived in a sample of $Z$ bosons produced in association with jets ($Z+$jets) \cite{ATLAS:2023lwk}. Due to the extreme rejection power of modern flavour tagger, a special technique so-called ``flip tagger'' is used for this last \gls{sf} in which the tagger is modified to have a reduced light-rejection.
  \item \gls{mc}-\gls{mc} \gls{sf} are then derived between the chosen Monte Carlo simulator for the training and other simulators or by changing the tuning \cite{ATL-PHYS-PUB-2020-009}. This dependency is measured by applying the same tagger to samples simulated with different generators, mainly \textsc{Pythia} \cite{SJOSTRAND2015159}, \textsc{Herwig} \cite{bellm2017herwig}, and \textsc{Sherpa} \cite{sherpa2.2paper} for variation to the parton shower and hadronisation and \textsc{MadGraph} for variation of the matrix element \cite{madgraph}. The decay chains of $b$- and $c$-hadrons in ATLAS is further simulated with the \textsc{EvtGen} package \cite{LANGE2001152}. 
  These effects are measured into \gls{sf} using the same technique as the data-\gls{mc} scale factors. For an alternative generator, the \gls{sf} of flavour $f$ is derived by composing the Data-\gls{mc} \gls{sf} with the nominal sample and the \gls{mc}-\gls{mc} \gls{sf} as: \[\textrm{SF}^f_{\textrm{Alternative}}(p_T) = \frac{\epsilon^f_{\textrm{Data}}(p_T)}{\epsilon^f_{\textrm{Nominal MC}}(p_T)} \times \frac{\epsilon^f_{\textrm{Nominal MC}}(p_T)}{\epsilon^f_{\textrm{Alternative MC}}(p_T)} = \frac{SF^f_{\textrm{data-MC}}(p_T)}{SF^f_{\textrm{MC-MC}}(p_T)}.\] 
\end{itemize}
These scale factors are applied in physics analyses as a per-jet weight. Some early studies of both scale factor types have been performed in Ref \cite{ATL-PLOT-FTAG-2023-01}, showing good agreement between the data and simulated performance of \gls{dl1d} and \gls{gn1}. Variations due to the change of generator are also found to be at most of 8\% with respect to the nominal choice.

\section{Conclusion}
This chapter introduces the main machine learning models developped for heavy-flavour jets identification in ATLAS during the period covering 2020 to 2024. Work carried out in and presented in this thesis includes the first training of the \gls{dl1d} model, including the \gls{dips} sub-tagger of the first time in the ATLAS software. \gls{dl1d} is found to have improved background rejections at a fixed working point for both $b$- and $c$-tagging compared to the at-the-time released tagger: \gls{dl1r}. Significant changes in addition to the development of this new tagger were made to the preprocessing pipeline of the \textsc{Umami} framework \cite{UmamiCite} and the architecture as well as the list of features used. Finally, the new family of taggers based on graph neural network for \gls{gn1} and transformers for \gls{gn2} is introduced, with the architecture adopted fully introduced as well as a presentation and comparison of the results obtained by the new methods. A discussion on the hyperparameter optimisation of \gls{gn2} is also included, introducing the possibilities of using a new infrastructure of the Kubeflow server managed by \gls{cern} as well as the relevance of the maximal update parametrisation for improving the search for optimal hyperparameters of \gls{gn2}. Significant contributions were made to the development of the \textsc{Salt} framework used to train GN-typed model were made to support these studies \cite{SaltCite}. 

%\begin{tcolorbox}[colback=oxfordblue!5,colframe=blue!40!black,title=Summary of the Chapter]
%In this chapter, the \gls{ftag} world is introduced. 
%\end{tcolorbox}